# 섹션 9: 이미지 최적화

> **학습 목표**: 이 장을 완료하면 Docker 이미지 크기를 90% 이상 줄이고, 빌드 시간을 80% 단축하며, 보안 취약점을 최소화할 수 있습니다.

**⏱️ 예상 학습 시간**: 3-4시간
**난이도**: ⭐⭐⭐☆☆ (3개/5개)

---

## 📚 목차
- [왜 이미지 최적화가 중요한가](#91-왜-이미지-최적화가-중요한가)
- [실생활 비유로 이해하기](#실생활-비유로-이해하기)
- [이미지 크기 분석](#92-이미지-크기-분석)
- [베이스 이미지 선택](#93-베이스-이미지-선택)
- [레이어 최적화](#94-레이어-최적화)
- [불필요한 파일 제거](#95-불필요한-파일-제거)
- [고급 최적화 기법](#96-고급-최적화-기법)
- [실전 최적화 예시](#97-실전-최적화-예시)
- [보안 스캔 및 검증](#98-보안-스캔-및-검증)
- [최적화 체크리스트](#99-최적화-체크리스트)
- [주니어 시나리오](#주니어-시나리오)
- [FAQ](#faq)
- [면접 질문 리스트](#면접-질문-리스트)
- [핵심 정리](#핵심-정리)
- [다음 단계](#910-다음-단계)

---

## 🌟 실생활 비유로 이해하기

### 비유 1: 이사짐 꾸리기
```
Docker 이미지 = 이사할 때 짐 꾸리기

최적화 전:
┌────────────────────────────┐
│ 이사짐 트럭 (10톤)         │
│ - 실제 필요한 물건: 2톤    │
│ - 안 쓰는 옷장: 1톤        │
│ - 중복된 그릇: 0.5톤       │
│ - 포장 박스만: 0.5톤       │
│ - 설명서/영수증: 6톤        │
└────────────────────────────┘
이사 비용: 100만원
이사 시간: 8시간

최적화 후:
┌────────────────────────────┐
│ 이사짐 트럭 (2톤)          │
│ - 실제 필요한 물건만: 2톤  │
│ (나머지는 모두 정리!)       │
└────────────────────────────┘
이사 비용: 20만원 (80% 절감!)
이사 시간: 1.5시간 (81% 단축!)
```

### 비유 2: 배달앱 음식 포장
```
멀티스테이지 빌드 = 배달 음식 포장 과정

❌ 비효율적: 조리 도구까지 배달
┌─────────────────────┐
│ 음식 + 냄비 + 가스레인지 │
│ + 칼 + 도마 + 앞치마   │
└─────────────────────┘
무게: 50kg
배달 시간: 2시간

✅ 효율적: 음식만 포장
┌─────────────────────┐
│ 완성된 음식만         │
└─────────────────────┘
무게: 1kg
배달 시간: 15분

멀티스테이지 빌드도 동일!
- 1단계: 빌드 도구로 요리 (컴파일)
- 2단계: 완성품만 포장 (바이너리만)
```

### 비유 3: 지하철 출근길
```
이미지 크기 = 출근 시 가방 크기

1.2GB 이미지 (큰 캐리어):
┌────────────────────┐
│ 🧳 대형 캐리어       │
│ - 지하철 탑승 느림   │
│ - 계단 이동 힘듦     │
│ - 사람들 불편       │
└────────────────────┘
출근 시간: 50분

50MB 이미지 (가벼운 백팩):
┌────────────────────┐
│ 🎒 백팩             │
│ - 빠른 이동         │
│ - 계단도 쉽게       │
│ - 기동성 최고       │
└────────────────────┘
출근 시간: 20분

배포도 마찬가지:
작은 이미지 = 빠른 배포!
```

### 비유 4: 스마트폰 앱 설치
```
이미지 다운로드 = 앱 설치

❌ 최적화 안 된 이미지 (1.2GB):
[■■■■■■■■■□] 90%... (5분 소요)
"아직도 설치 중..."
"데이터 너무 많이 써..."
사용자: 😡 짜증

✅ 최적화된 이미지 (50MB):
[■■■■■■■■■■] 100%! (10초 완료)
"순식간에 완료!"
"데이터 절약!"
사용자: 😊 만족

실제 서비스:
- 큰 이미지 = 사용자 이탈
- 작은 이미지 = 사용자 만족
```

### 비유 5: 아파트 평수
```
베이스 이미지 선택 = 아파트 평수 선택

Ubuntu (72MB) = 100평 아파트
┌────────────────────────────┐
│ 거실, 방4, 주방, 창고, 다락... │
│ "혼자 사는데 너무 넓어!"     │
└────────────────────────────┘
관리비: 월 50만원

Alpine (7MB) = 20평 원룸
┌────────────────────────────┐
│ 필요한 공간만!              │
│ "딱 맞아!"                  │
└────────────────────────────┘
관리비: 월 5만원

Scratch (0MB) = 텐트
┌────────────────────────────┐
│ 정말 최소한만!              │
│ (정적 바이너리만 가능)       │
└────────────────────────────┘
관리비: 월 0원
```

### 🎯 종합 비교표
```
┌─────────────┬──────────────┬──────────────┬──────────────┐
│ 기술        │ 실생활 비유   │ 효과         │ 중요도       │
├─────────────┼──────────────┼──────────────┼──────────────┤
│ 베이스 선택  │ 아파트 평수   │ 크기 90% 감소 │ ⭐⭐⭐⭐⭐    │
│ 멀티스테이지 │ 음식 포장     │ 크기 70% 감소 │ ⭐⭐⭐⭐⭐    │
│ .dockerignore│ 이사짐 정리   │ 빌드 50% 빠름 │ ⭐⭐⭐⭐     │
│ 레이어 체이닝│ 짐 압축 포장  │ 크기 30% 감소 │ ⭐⭐⭐       │
│ 캐시 정리    │ 포장재 제거   │ 크기 20% 감소 │ ⭐⭐⭐       │
└─────────────┴──────────────┴──────────────┴──────────────┘
```

---

## 📊 수치로 보는 효과

**Docker 이미지 최적화의 실제 성과 지표**

| 지표 | Before (최적화 전) | After (최적화 후) | 개선율 |
|------|-------------------|------------------|--------|
| 이미지 크기 (Node.js) | 1.2GB | 50MB | **96%↓** |
| 이미지 크기 (Go) | 800MB | 10MB | **99%↓** |
| 이미지 크기 (Python) | 1.1GB | 80MB | **93%↓** |
| 빌드 시간 (초기) | 180초 | 120초 | **33%↓** |
| 빌드 시간 (재빌드) | 180초 | 30초 | **83%↓** |
| 다운로드 시간 | 96초 | 4초 | **96%↓** |
| 배포 시간 (100개 컨테이너) | 60분 | 5분 | **92%↓** |
| 클라우드 스토리지 비용 (월) | $120 | $10 | **92%↓** |
| 네트워크 대역폭 (100개 배포) | 120GB | 5GB | **96%↓** |
| 보안 취약점 개수 | 50개 | 5개 | **90%↓** |
| 컨테이너 시작 시간 | 8.5초 | 2.1초 | **75%↓** |
| 메모리 사용량 | 512MB | 128MB | **75%↓** |

### 💰 비용 절감 사례

**시나리오: 스타트업이 1000개 컨테이너 운영**

```
최적화 전 (1.2GB/컨테이너):
├─ 스토리지 비용: $120/월
├─ 네트워크 비용: $80/월
├─ 배포 시간: 16분/회 × 10회/일 = 2.6시간/일
└─ 개발자 대기 시간 비용: $50/시간 × 2.6시간 = $130/일

월간 총 비용: $200 (인프라) + $3,900 (인건비) = $4,100

최적화 후 (50MB/컨테이너):
├─ 스토리지 비용: $10/월
├─ 네트워크 비용: $5/월
├─ 배포 시간: 40초/회 × 10회/일 = 6.7분/일
└─ 개발자 대기 시간 비용: $50/시간 × 0.1시간 = $5/일

월간 총 비용: $15 (인프라) + $150 (인건비) = $165

연간 절감액: ($4,100 - $165) × 12 = $47,220 💰
```

### 🚀 성능 개선 사례

**대규모 마이크로서비스 배포 시나리오**

```
[100개 컨테이너 동시 배포 비교]

최적화 전 (1.2GB 이미지):
┌────────────────────────────────────┐
│ 1. 이미지 Pull: 5분              │
│ 2. 컨테이너 시작: 850ms × 100    │
│ 3. 헬스체크 대기: 30초            │
│                                    │
│ 총 배포 시간: 7분 15초            │
│ 네트워크 사용: 120GB              │
└────────────────────────────────────┘

최적화 후 (50MB 이미지):
┌────────────────────────────────────┐
│ 1. 이미지 Pull: 10초              │
│ 2. 컨테이너 시작: 210ms × 100    │
│ 3. 헬스체크 대기: 10초            │
│                                    │
│ 총 배포 시간: 41초                │
│ 네트워크 사용: 5GB                │
└────────────────────────────────────┘

결과: 10.6배 빠른 배포! 🚀
```

### 🛡️ 보안 개선 효과

| 베이스 이미지 | 패키지 수 | 알려진 취약점 | 공격 표면 | 보안 등급 |
|--------------|----------|--------------|----------|-----------|
| Ubuntu 20.04 (72MB) | 500+ | 50+ | 200+ 바이너리 | ⭐⭐⭐ |
| Debian Slim (27MB) | 150+ | 20+ | 80+ 바이너리 | ⭐⭐⭐⭐ |
| Alpine (7MB) | 20+ | 5+ | 30+ 바이너리 | ⭐⭐⭐⭐⭐ |
| Distroless (2MB) | 5+ | 1+ | 5+ 바이너리 | ⭐⭐⭐⭐⭐⭐ |

**위험도 감소율: Ubuntu → Distroless = 98% 감소**

---

## 9.1 왜 이미지 최적화가 중요한가?

### 9.1.1 비용 절감

```
[시나리오: 1000개 컨테이너 운영]

최적화 전: 1.2GB/컨테이너
총 스토리지: 1.2TB
클라우드 비용 ($0.10/GB/월): $120/월

최적화 후: 100MB/컨테이너
총 스토리지: 100GB
클라우드 비용 ($0.10/GB/월): $10/월

연간 절감: $1,320 💰
```

### 9.1.2 배포 속도 향상

```
[이미지 크기별 다운로드 시간]

네트워크 속도: 100Mbps (일반 기업 환경)

1.2GB 이미지: 96초
500MB 이미지: 40초
100MB 이미지: 8초  ⚡
50MB 이미지: 4초

컨테이너 10개 동시 배포:
1.2GB → 16분
50MB → 40초 (24배 빠름!)
```

### 9.1.3 보안 향상

```
[공격 표면 비교]

Ubuntu 기반 (1.2GB):
- 패키지 수: 500+
- 알려진 취약점: 50+
- 공격 가능한 바이너리: 200+

Alpine 기반 (50MB):
- 패키지 수: 20+
- 알려진 취약점: 5+
- 공격 가능한 바이너리: 30+

Distroless (10MB):
- 패키지 수: 5+
- 알려진 취약점: 1+
- 공격 가능한 바이너리: 5+

위험도: 90% 감소 🛡️
```

---

## 9.2 이미지 크기 분석

### 9.2.1 현재 이미지 분석 도구

#### docker images - 기본 확인

```bash
docker images
# REPOSITORY    TAG       IMAGE ID       CREATED         SIZE
# myapp         latest    abc123def456   2 minutes ago   1.2GB
```

#### docker history - 레이어별 크기

```bash
docker history myapp:latest

# IMAGE          CREATED       CREATED BY                                      SIZE
# abc123def456   2 mins ago    CMD ["node" "server.js"]                        0B
# def456789012   2 mins ago    COPY . /app                                     150MB
# 789012345678   3 mins ago    RUN npm install                                 500MB
# 345678901234   5 mins ago    COPY package*.json /app/                        5KB
# 901234567890   10 mins ago   WORKDIR /app                                    0B
# 567890123456   1 day ago     /bin/sh -c #(nop)  CMD ["node"]                 0B
# ...            ...           node:18 base layers                             550MB
```

**분석:**
```
총 크기: 1.2GB
├─ Node.js 베이스 이미지: 550MB (46%)
├─ npm install (node_modules): 500MB (42%)
├─ 소스 코드 복사: 150MB (12%)
└─ 기타: 5KB (0%)

개선 포인트:
1. ⚠️ node:18-alpine 사용 (550MB → 120MB)
2. ⚠️ 멀티스테이지로 devDependencies 제거 (500MB → 100MB)
3. ⚠️ .dockerignore로 불필요한 파일 제외 (150MB → 10MB)
```

#### dive - 레이어 탐색 도구

**설치:**

```bash
# macOS
brew install dive

# Linux
wget https://github.com/wagoodman/dive/releases/download/v0.11.0/dive_0.11.0_linux_amd64.deb
sudo dpkg -i dive_0.11.0_linux_amd64.deb

# Windows (WSL2)
wget https://github.com/wagoodman/dive/releases/download/v0.11.0/dive_0.11.0_linux_amd64.tar.gz
tar -xzf dive_0.11.0_linux_amd64.tar.gz
sudo mv dive /usr/local/bin/
```

**사용:**

```bash
dive myapp:latest
```

**출력 예시:**

```
┌─ Layers ────────────────────────────────────────────┐
│ Cmp   Size  Command                                  │
│     550 MB  FROM node:18                             │
│       0 B   WORKDIR /app                             │
│      5 KB   COPY package*.json /app/                 │
│     500 MB  RUN npm install                          │
│     150 MB  COPY . /app                              │
│       0 B   CMD ["node", "server.js"]                │
└─────────────────────────────────────────────────────┘

Efficiency Score: 42% (F)
  - 700 MB wasted space
  - Recommendations:
    • Use alpine base image
    • Implement multi-stage build
    • Remove devDependencies
```

---

### 9.2.2 불필요한 파일 찾기

```bash
# 컨테이너에서 큰 파일 찾기
docker run --rm myapp:latest find / -type f -size +10M -exec ls -lh {} \; 2>/dev/null

# 출력 예시:
# -rw-r--r-- 1 root root 50M /app/node_modules/puppeteer/.local-chromium/...
# -rw-r--r-- 1 root root 30M /app/videos/demo.mp4
# -rw-r--r-- 1 root root 20M /usr/share/doc/...

# 분석: puppeteer 크롬 바이너리 불필요, 비디오 파일은 외부 저장소로
```

---

## 9.3 베이스 이미지 선택

### 9.3.1 베이스 이미지 비교

| 이미지 | 크기 | 패키지 관리자 | 사용 사례 | 보안 |
|--------|------|---------------|-----------|------|
| **Ubuntu 20.04** | 72MB | apt | 범용, 많은 도구 필요 | ⭐⭐⭐ |
| **Debian Slim** | 27MB | apt | Ubuntu 대체, 더 작음 | ⭐⭐⭐⭐ |
| **Alpine Linux** | 7MB | apk | 최소 크기, 프로덕션 | ⭐⭐⭐⭐⭐ |
| **Distroless** | 2MB | 없음 | 정적 바이너리만 | ⭐⭐⭐⭐⭐⭐ |
| **Scratch** | 0MB | 없음 | 완전 정적 바이너리 | ⭐⭐⭐⭐⭐⭐ |

### 9.3.2 언어별 권장 이미지

#### Node.js

```dockerfile
# ❌ 비권장: 기본 이미지 (1GB)
FROM node:18

# ✅ 권장: Alpine (120MB)
FROM node:18-alpine

# ✅✅ 더 나은 선택: 멀티스테이지 + Alpine (50MB)
FROM node:18-alpine AS builder
# 빌드 작업
FROM node:18-alpine
COPY --from=builder ...
```

#### Python

```dockerfile
# ❌ 비권장: 전체 이미지 (1GB)
FROM python:3.11

# ✅ 권장: Slim (130MB)
FROM python:3.11-slim

# ✅✅ 더 나은 선택: Alpine (50MB)
# 주의: 일부 C 확장 빌드 필요
FROM python:3.11-alpine
RUN apk add --no-cache gcc musl-dev
```

#### Go

```dockerfile
# ❌ 비권장: 개발 이미지 (800MB)
FROM golang:1.21

# ✅ 권장: 멀티스테이지 + Alpine (20MB)
FROM golang:1.21-alpine AS builder
RUN go build -o app .

FROM alpine:3.18
COPY --from=builder /app/app .

# ✅✅ 최고 선택: Distroless (10MB)
FROM golang:1.21 AS builder
RUN CGO_ENABLED=0 go build -o app .

FROM gcr.io/distroless/static-debian11
COPY --from=builder /app/app /
```

#### Java

```dockerfile
# ❌ 비권장: JDK (500MB)
FROM openjdk:17

# ✅ 권장: JRE Alpine (200MB)
FROM eclipse-temurin:17-jre-alpine

# ✅✅ 더 나은 선택: 커스텀 JRE (100MB)
FROM eclipse-temurin:17-jdk-alpine AS builder
RUN jlink \
    --add-modules java.base,java.logging,java.sql \
    --output /custom-jre \
    --compress=2 \
    --no-header-files \
    --no-man-pages

FROM alpine:3.18
COPY --from=builder /custom-jre /opt/jre
ENV PATH="/opt/jre/bin:${PATH}"
```

---

### 9.3.3 Alpine Linux 주의사항

#### musl libc vs glibc

```dockerfile
# ❌ 문제: glibc 기반 바이너리가 Alpine에서 작동 안 함
FROM alpine:3.18
COPY my-prebuilt-binary /usr/local/bin/
RUN my-prebuilt-binary
# 오류: Error loading shared library libc.so.6

# ✅ 해결 1: glibc 호환 레이어 추가
FROM alpine:3.18
RUN apk add --no-cache libc6-compat
COPY my-prebuilt-binary /usr/local/bin/

# ✅ 해결 2: Alpine에서 직접 빌드
FROM alpine:3.18
RUN apk add --no-cache build-base
COPY src/ .
RUN gcc -o my-binary main.c
```

#### DNS 이슈

```dockerfile
# ❌ 문제: Alpine에서 DNS 해석 실패
FROM alpine:3.18
RUN wget https://example.com/file.tar.gz
# 오류: bad address 'example.com'

# ✅ 해결: ca-certificates 설치
FROM alpine:3.18
RUN apk add --no-cache ca-certificates
RUN wget https://example.com/file.tar.gz
```

---

## 9.4 레이어 최적화

### 9.4.1 RUN 명령어 체이닝

#### ❌ 비효율적인 방식

```dockerfile
FROM ubuntu:20.04

RUN apt-get update
RUN apt-get install -y nginx
RUN apt-get install -y curl
RUN apt-get install -y vim

# 결과:
# Layer 1: 50MB (apt-get update)
# Layer 2: 60MB (nginx + 캐시)
# Layer 3: 10MB (curl + 캐시)
# Layer 4: 15MB (vim + 캐시)
# 총: 135MB
```

#### ✅ 최적화된 방식

```dockerfile
FROM ubuntu:20.04

RUN apt-get update && apt-get install -y \
    nginx \
    curl \
    vim \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*

# 결과:
# Layer 1: 85MB (패키지 + 캐시 정리 후)
# 총: 85MB (37% 절감!)
```

---

### 9.4.2 캐시 정리

#### Node.js

```dockerfile
# ❌ 캐시 남김
RUN npm install
# node_modules/.cache/ 폴더가 이미지에 포함 (50MB+)

# ✅ 캐시 정리
RUN npm ci --only=production \
 && npm cache clean --force
# 캐시 제거 후 크기 감소
```

#### Python

```dockerfile
# ❌ 캐시 남김
RUN pip install -r requirements.txt
# ~/.cache/pip/ 폴더 포함 (100MB+)

# ✅ 캐시 정리
RUN pip install --no-cache-dir -r requirements.txt
```

#### Ubuntu/Debian

```dockerfile
# ❌ 캐시 남김
RUN apt-get update && apt-get install -y nginx
# /var/lib/apt/lists/ 캐시 포함 (50MB+)

# ✅ 캐시 정리
RUN apt-get update \
 && apt-get install -y --no-install-recommends nginx \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*
```

#### Alpine Linux

```dockerfile
# ❌ 캐시 남김
RUN apk add nginx

# ✅ 자동 캐시 정리
RUN apk add --no-cache nginx
# Alpine은 자동으로 캐시 정리
```

---

### 9.4.3 레이어 순서 최적화

**원칙: 자주 변경되는 것을 아래로**

```dockerfile
# ❌ 비효율적인 순서
FROM node:18-alpine

WORKDIR /app

# 1. 소스 복사 (자주 변경됨)
COPY . .

# 2. 의존성 설치
RUN npm install

# 문제: 소스가 변경될 때마다 npm install 재실행!
```

```dockerfile
# ✅ 최적화된 순서
FROM node:18-alpine

WORKDIR /app

# 1. 의존성 파일만 복사
COPY package*.json ./

# 2. 의존성 설치 (캐시됨!)
RUN npm ci --only=production

# 3. 소스 복사 (자주 변경됨 - 마지막에)
COPY . .

# 효과: package.json 변경 없으면 npm ci는 캐시 사용
```

**실제 효과:**

```bash
# 첫 번째 빌드
[+] Building 120s
 => COPY package*.json ./        0.1s
 => RUN npm ci                   90.0s  ⏳
 => COPY . .                     0.5s
 => RUN npm run build            29.4s

# 소스만 변경 후 두 번째 빌드
[+] Building 30s
 => COPY package*.json ./        CACHED ✅
 => RUN npm ci                   CACHED ✅
 => COPY . .                     0.6s
 => RUN npm run build            29.5s

# 빌드 시간: 120s → 30s (75% 단축!)
```

---

## 9.5 불필요한 파일 제거

### 9.5.1 .dockerignore 작성

**.dockerignore 템플릿 (Node.js):**

```
# 의존성
node_modules/
npm-debug.log
yarn-debug.log
yarn-error.log

# 빌드 출력
dist/
build/
.next/
out/

# 테스트
coverage/
.nyc_output/
__tests__/
*.test.js
*.spec.js

# 환경 설정
.env
.env.local
.env.*.local

# Git
.git/
.gitignore
.gitattributes

# IDE
.vscode/
.idea/
*.swp
*.swo
.DS_Store
Thumbs.db

# 문서
README.md
CHANGELOG.md
docs/
*.pdf

# CI/CD
.github/
.gitlab-ci.yml
Jenkinsfile

# 로그
logs/
*.log
```

**.dockerignore 템플릿 (Python):**

```
# 가상 환경
venv/
env/
ENV/
.venv/

# 컴파일된 파일
__pycache__/
*.py[cod]
*.so
*.egg
*.egg-info/
dist/
build/

# 테스트
.pytest_cache/
.tox/
.coverage
htmlcov/

# 환경 설정
.env
*.env

# Git 및 IDE
.git/
.vscode/
.idea/

# 문서
*.md
docs/
```

**효과 측정:**

```bash
# .dockerignore 없을 때
docker build -t myapp:before .
# Sending build context to Docker daemon  1.2GB

# .dockerignore 추가 후
docker build -t myapp:after .
# Sending build context to Docker daemon  5.8MB

# 전송량 감소: 1.2GB → 5.8MB (99.5% 감소!)
```

---

### 9.5.2 빌드 중 정리

```dockerfile
# ❌ 임시 파일이 레이어에 남음
FROM ubuntu:20.04
RUN wget https://example.com/large-file.tar.gz
RUN tar -xzf large-file.tar.gz
RUN rm large-file.tar.gz
# 문제: large-file.tar.gz가 중간 레이어에 존재 (이미지 크기 증가)

# ✅ 한 RUN 명령어에서 다운로드 → 압축해제 → 삭제
FROM ubuntu:20.04
RUN wget https://example.com/large-file.tar.gz \
 && tar -xzf large-file.tar.gz \
 && rm large-file.tar.gz
# 장점: tar.gz 파일이 최종 레이어에 없음

# ✅✅ 파이프로 직접 전달 (임시 파일 없음)
FROM ubuntu:20.04
RUN wget -qO- https://example.com/large-file.tar.gz | tar -xz
# 최고의 방법: 디스크 I/O 절약, 레이어 최소화
```

**크기 비교:**

```
방법 1 (별도 RUN):     200MB (tar.gz 100MB + 압축해제 100MB)
방법 2 (체이닝):       100MB (압축해제만)
방법 3 (파이프):       100MB (압축해제만, I/O 빠름)
```

---

## 9.6 고급 최적화 기법

### 9.6.1 바이너리 스트립 (Go, C/C++)

```dockerfile
# ==================================
# 빌드 단계
# ==================================
FROM golang:1.21 AS builder

WORKDIR /build
COPY . .

# ❌ 일반 빌드 (디버그 심볼 포함)
RUN go build -o myapp .
# 크기: 15MB

# ✅ 최적화 빌드 (디버그 심볼 제거)
RUN CGO_ENABLED=0 GOOS=linux go build \
    -ldflags="-w -s" \
    -a -installsuffix cgo \
    -o myapp .
# 크기: 8MB (47% 감소!)

# 설명:
# -ldflags="-w -s"
#   -w: DWARF 디버그 정보 제거
#   -s: 심볼 테이블 제거
# CGO_ENABLED=0: 정적 링크 (C 라이브러리 불필요)

# ==================================
# 런타임 단계
# ==================================
FROM scratch
COPY --from=builder /build/myapp /myapp
CMD ["/myapp"]
```

---

### 9.6.2 UPX 압축 (극한의 최적화)

**UPX (Ultimate Packer for eXecutables):**
- 실행 파일 압축 도구
- 실행 시 자동 압축 해제
- 크기 50-70% 감소

```dockerfile
# ==================================
# 빌드 및 압축
# ==================================
FROM golang:1.21-alpine AS builder

RUN apk add --no-cache upx

WORKDIR /build
COPY . .

# 빌드
RUN CGO_ENABLED=0 go build -ldflags="-w -s" -o myapp .

# UPX 압축
RUN upx --best --lzma myapp
# Before: 8MB
# After: 2.5MB (69% 감소!)

# ==================================
# 런타임
# ==================================
FROM scratch
COPY --from=builder /build/myapp /myapp
CMD ["/myapp"]
```

**주의사항:**

```
장점:
✅ 이미지 크기 대폭 감소
✅ 다운로드 속도 향상

단점:
❌ 실행 시 압축 해제 시간 (0.1초 정도)
❌ 안티바이러스가 오탐지할 수 있음
❌ 메모리 사용량 약간 증가

추천:
- 배포 빈도가 높을 때: ✅ 사용
- 마이크로서비스 (컨테이너 수백 개): ✅ 사용
- 단일 서버 운영: ❌ 불필요
```

---

### 9.6.3 Java Jlink - 커스텀 JRE

**기본 JRE vs 커스텀 JRE:**

```dockerfile
# ==================================
# Stage 1: 커스텀 JRE 생성
# ==================================
FROM eclipse-temurin:17-jdk-alpine AS jre-builder

# 애플리케이션이 사용하는 모듈만 포함
RUN jlink \
    --add-modules java.base,java.logging,java.sql,java.naming,java.desktop,java.management,java.security.jgss,java.instrument \
    --output /custom-jre \
    --compress=2 \
    --strip-debug \
    --no-header-files \
    --no-man-pages

# ==================================
# Stage 2: 애플리케이션 빌드
# ==================================
FROM gradle:8.5-jdk17 AS builder

WORKDIR /build
COPY . .
RUN gradle build -x test --no-daemon

# ==================================
# Stage 3: 런타임
# ==================================
FROM alpine:3.18

# 커스텀 JRE 복사
COPY --from=jre-builder /custom-jre /opt/jre
ENV PATH="/opt/jre/bin:${PATH}"

WORKDIR /app
COPY --from=builder /build/build/libs/*.jar app.jar

EXPOSE 8080
CMD ["java", "-jar", "app.jar"]
```

**크기 비교:**

| JRE 타입 | 크기 | 포함 모듈 |
|----------|------|-----------|
| 전체 JRE | 200MB | 모든 Java 모듈 |
| 커스텀 JRE (위 예시) | 60MB | 필요한 8개 모듈만 |
| 최소 JRE (java.base만) | 40MB | 기본 모듈만 |

---

### 9.6.4 멀티 아키텍처 빌드

**상황:**
- Apple Silicon (ARM64) 개발자가 빌드
- 배포 서버는 AMD64 (x86_64)

```dockerfile
# ==================================
# 멀티 아키텍처 지원
# ==================================
FROM --platform=$BUILDPLATFORM golang:1.21-alpine AS builder

ARG TARGETPLATFORM
ARG BUILDPLATFORM
ARG TARGETOS
ARG TARGETARCH

WORKDIR /build
COPY . .

# 타겟 아키텍처에 맞게 빌드
RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o myapp .

# ==================================
# 런타임
# ==================================
FROM alpine:3.18

COPY --from=builder /build/myapp /usr/local/bin/
CMD ["myapp"]
```

**빌드 명령어:**

```bash
# 단일 아키텍처
docker build -t myapp:amd64 --platform linux/amd64 .
docker build -t myapp:arm64 --platform linux/arm64 .

# 멀티 아키텍처 동시 빌드
docker buildx build \
  --platform linux/amd64,linux/arm64,linux/arm/v7 \
  -t myregistry.com/myapp:latest \
  --push \
  .

# 실행 (자동으로 현재 플랫폼에 맞는 이미지 선택)
docker run myregistry.com/myapp:latest
```

---

## 9.7 실전 최적화 예시

### 9.7.1 Node.js 애플리케이션 완전 최적화

**최적화 전 (1.2GB):**

```dockerfile
FROM node:18

WORKDIR /app
COPY . .

RUN npm install
RUN npm run build

CMD ["node", "dist/server.js"]
```

**최적화 후 (50MB):**

```dockerfile
# ==================================
# Stage 1: 의존성
# ==================================
FROM node:18-alpine AS deps

WORKDIR /deps

COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# ==================================
# Stage 2: 빌드
# ==================================
FROM node:18-alpine AS builder

WORKDIR /build

COPY package*.json ./
RUN npm ci && npm cache clean --force

COPY . .
RUN npm run build

# ==================================
# Stage 3: 런타임
# ==================================
FROM node:18-alpine

ENV NODE_ENV=production

# 보안
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

# 필수 런타임 라이브러리만
RUN apk add --no-cache dumb-init

WORKDIR /app

# 프로덕션 의존성
COPY --from=deps --chown=nodejs:nodejs /deps/node_modules ./node_modules

# 빌드 결과물
COPY --from=builder --chown=nodejs:nodejs /build/dist ./dist
COPY --chown=nodejs:nodejs package*.json ./

USER nodejs

EXPOSE 3000

# dumb-init으로 시그널 올바르게 전달
CMD ["dumb-init", "node", "dist/server.js"]
```

**.dockerignore:**

```
node_modules/
npm-debug.log
.git/
.vscode/
.env*
*.md
dist/
build/
coverage/
__tests__/
*.test.js
```

**최적화 결과:**

```
항목                 최적화 전      최적화 후      개선율
──────────────────────────────────────────────────────
이미지 크기           1.2GB         50MB           96% ↓
빌드 시간 (초기)      180초         120초          33% ↓
빌드 시간 (재빌드)    180초         30초           83% ↓
레이어 수             15개          8개            47% ↓
다운로드 시간         96초          4초            96% ↓
취약점 개수           42개          5개            88% ↓
```

---

### 9.7.2 Python Flask API 완전 최적화

**최적화 전 (1.1GB):**

```dockerfile
FROM python:3.11

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

**최적화 후 (80MB):**

```dockerfile
# ==================================
# Stage 1: 빌드
# ==================================
FROM python:3.11-alpine AS builder

# 빌드 도구
RUN apk add --no-cache \
    gcc \
    musl-dev \
    libffi-dev \
    openssl-dev \
    postgresql-dev

# 가상 환경 생성
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# 의존성 설치
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ==================================
# Stage 2: 런타임
# ==================================
FROM python:3.11-alpine

# 런타임 라이브러리만
RUN apk add --no-cache \
    libpq \
    libffi \
    openssl

# 가상 환경 복사
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Python 최적화
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONOPTIMIZE=2

# 보안
RUN addgroup -g 1001 appgroup && \
    adduser -D -u 1001 -G appgroup appuser

WORKDIR /app
COPY --chown=appuser:appgroup . .

USER appuser

EXPOSE 5000

# Gunicorn으로 프로덕션 서빙
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "app:app"]
```

**requirements.txt 분리 (선택사항):**

```
# requirements-base.txt (필수)
Flask==3.0.0
gunicorn==21.2.0
psycopg2-binary==2.9.9

# requirements-dev.txt (개발만)
pytest==7.4.3
black==23.12.0
flake8==6.1.0
```

```dockerfile
# 프로덕션용
RUN pip install --no-cache-dir -r requirements-base.txt

# 개발용 (조건부)
ARG ENVIRONMENT=production
RUN if [ "$ENVIRONMENT" = "development" ]; then \
      pip install --no-cache-dir -r requirements-dev.txt; \
    fi
```

---

## 9.8 보안 스캔 및 검증

### 9.8.1 Trivy - 취약점 스캔

**설치:**

```bash
# macOS
brew install aquasecurity/trivy/trivy

# Linux
wget https://github.com/aquasecurity/trivy/releases/download/v0.48.0/trivy_0.48.0_Linux-64bit.tar.gz
tar -xzf trivy_0.48.0_Linux-64bit.tar.gz
sudo mv trivy /usr/local/bin/
```

**사용:**

```bash
# 이미지 스캔
trivy image myapp:latest

# 출력 예시:
# myapp:latest (alpine 3.18.4)
# ==========================
# Total: 5 (UNKNOWN: 0, LOW: 2, MEDIUM: 2, HIGH: 1, CRITICAL: 0)
#
# ┌──────────────┬────────────┬──────────┬───────────────┬───────────────────┐
# │   Library    │ Vulnerability │ Severity │ Installed Ver │   Fixed Version   │
# ├──────────────┼────────────┼──────────┼───────────────┼───────────────────┤
# │ openssl      │ CVE-2023-123  │ HIGH     │ 3.0.10-r0     │ 3.0.12-r0         │
# │ curl         │ CVE-2023-456  │ MEDIUM   │ 8.4.0-r0      │ 8.5.0-r0          │
# └──────────────┴────────────┴──────────┴───────────────┴───────────────────┘

# 심각도별 필터
trivy image --severity HIGH,CRITICAL myapp:latest

# JSON 출력
trivy image -f json -o results.json myapp:latest
```

---

### 9.8.2 Docker Scan (Snyk 통합)

```bash
# 로그인
docker scan --login

# 스캔
docker scan myapp:latest

# 출력:
# Testing myapp:latest...
#
# ✗ High severity vulnerability found in openssl
#   Description: Buffer Overflow
#   Info: https://snyk.io/vuln/SNYK-ALPINE318-OPENSSL-123456
#   Introduced through: openssl@3.0.10-r0
#   Fixed in: 3.0.12-r0
#
# Tested 150 dependencies for known issues, found 5 issues.
```

---

### 9.8.3 CI/CD 통합

**GitHub Actions:**

```yaml
name: Build and Scan

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build Docker Image
        run: docker build -t myapp:${{ github.sha }} .

      - name: Run Trivy Scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: myapp:${{ github.sha }}
          severity: 'HIGH,CRITICAL'
          exit-code: '1'  # 취약점 발견 시 빌드 실패

      - name: Check Image Size
        run: |
          SIZE=$(docker images myapp:${{ github.sha }} --format "{{.Size}}")
          echo "Image size: $SIZE"

          # 100MB 초과 시 경고
          SIZE_MB=$(echo $SIZE | sed 's/MB//')
          if [ $SIZE_MB -gt 100 ]; then
            echo "Warning: Image size exceeds 100MB!"
            exit 1
          fi
```

---

## 9.9 최적화 체크리스트

### ✅ 이미지 크기

- [ ] Alpine 또는 Distroless 베이스 이미지 사용
- [ ] 멀티스테이지 빌드로 빌드 도구 제거
- [ ] .dockerignore 파일로 불필요한 파일 제외
- [ ] 패키지 관리자 캐시 정리 (`--no-cache`, `--no-cache-dir`)
- [ ] RUN 명령어 체이닝으로 레이어 최소화

### ✅ 빌드 성능

- [ ] 레이어 캐싱 최적화 (자주 변경되는 것은 하단 배치)
- [ ] 의존성 파일 먼저 복사 (package.json, requirements.txt)
- [ ] BuildKit 활성화 (`DOCKER_BUILDKIT=1`)
- [ ] 병렬 빌드 활용 (멀티스테이지)

### ✅ 보안

- [ ] root가 아닌 사용자로 실행 (`USER`)
- [ ] 취약점 스캔 (Trivy, Snyk)
- [ ] 최신 베이스 이미지 사용 (`node:18.19.0` 등 명확한 버전)
- [ ] 민감 정보 제외 (.env, credentials)

### ✅ 런타임 효율

- [ ] 프로덕션 의존성만 설치 (`--only=production`, `--production`)
- [ ] 정적 바이너리 사용 (Go: `CGO_ENABLED=0`)
- [ ] 압축 활성화 (UPX, gzip)
- [ ] 헬스체크 추가 (`HEALTHCHECK`)

---

---

## 👨‍💻 주니어 시나리오

### 시나리오 1: "왜 이미지가 이렇게 커요?"

**상황**: 주니어 개발자가 첫 Node.js 애플리케이션을 Docker로 배포했는데, 이미지 크기가 1.5GB나 됩니다.

```dockerfile
# ❌ 주니어가 작성한 Dockerfile
FROM node:18

WORKDIR /app
COPY . .
RUN npm install
RUN npm run build

CMD ["node", "dist/server.js"]
```

**문제점**:
- 문제 1: node:18 전체 이미지 사용 (950MB)
- 문제 2: .git, node_modules, 테스트 파일 등 모두 포함
- 문제 3: devDependencies까지 설치
- 문제 4: 빌드 산출물과 빌드 도구가 함께 있음
- 왜 이 문제가 발생하는가: 기본 예제를 그대로 따라 했고, 이미지 크기를 신경 쓰지 않음

**해결책**:
```dockerfile
# ✅ 올바른 Dockerfile
# Stage 1: 의존성
FROM node:18-alpine AS deps
WORKDIR /deps
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# Stage 2: 빌드
FROM node:18-alpine AS builder
WORKDIR /build
COPY package*.json ./
RUN npm ci && npm cache clean --force
COPY . .
RUN npm run build

# Stage 3: 런타임
FROM node:18-alpine
ENV NODE_ENV=production

RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

# 프로덕션 의존성만
COPY --from=deps --chown=nodejs:nodejs /deps/node_modules ./node_modules

# 빌드 결과물만
COPY --from=builder --chown=nodejs:nodejs /build/dist ./dist
COPY --chown=nodejs:nodejs package*.json ./

USER nodejs
EXPOSE 3000

CMD ["node", "dist/server.js"]
```

**.dockerignore 추가**:
```
node_modules/
.git/
.env*
*.md
dist/
build/
coverage/
.vscode/
__tests__/
*.test.js
```

**배운 점**:
- 💡 팁 1: 항상 Alpine 기반 이미지를 먼저 고려하세요 (node:18-alpine)
- 💡 팁 2: 멀티스테이지 빌드로 빌드 도구와 런타임을 분리하세요
- 💡 팁 3: .dockerignore는 필수입니다 (1.5GB → 50MB로 97% 감소!)
- 💡 팁 4: --only=production으로 프로덕션 의존성만 설치하세요

---

### 시나리오 2: "빌드가 너무 느려요!"

**상황**: 코드를 조금만 수정해도 매번 npm install이 10분씩 걸립니다.

```dockerfile
# ❌ 캐시를 활용하지 못하는 Dockerfile
FROM node:18-alpine

WORKDIR /app

# 모든 파일을 먼저 복사
COPY . .

# 의존성 설치 (항상 재실행됨!)
RUN npm install

CMD ["npm", "start"]
```

**문제점**:
- 문제 1: 소스 파일이 변경되면 COPY . . 레이어가 변경됨
- 문제 2: 레이어가 변경되면 그 이후 모든 레이어도 재실행됨
- 문제 3: 결과적으로 npm install이 매번 실행됨
- 왜 이 문제가 발생하는가: Docker 레이어 캐싱 원리를 몰랐음

**해결책**:
```dockerfile
# ✅ 캐시를 최대한 활용하는 Dockerfile
FROM node:18-alpine

WORKDIR /app

# 1단계: 의존성 파일만 먼저 복사 (자주 변경되지 않음)
COPY package*.json ./

# 2단계: 의존성 설치 (package.json 변경시에만 재실행)
RUN npm ci --only=production && npm cache clean --force

# 3단계: 소스 복사 (자주 변경됨 - 마지막에)
COPY . .

CMD ["npm", "start"]
```

**실제 효과**:
```bash
# 첫 빌드
$ docker build -t myapp .
[+] Building 180.0s
 => COPY package*.json          0.1s
 => RUN npm ci                  150.0s  ⏳
 => COPY . .                     0.5s
 => DONE                        180.0s

# 소스만 수정 후 재빌드
$ docker build -t myapp .
[+] Building 2.0s
 => COPY package*.json          CACHED ✅
 => RUN npm ci                  CACHED ✅
 => COPY . .                     0.6s
 => DONE                         2.0s

# 빌드 시간: 180초 → 2초 (99% 단축!)
```

**배운 점**:
- 💡 팁 1: 자주 변경되지 않는 것을 위에, 자주 변경되는 것을 아래에 배치
- 💡 팁 2: package.json만 먼저 복사하고 의존성 설치
- 💡 팁 3: 소스 코드는 가장 마지막에 복사
- 💡 팁 4: npm ci를 사용하면 npm install보다 빠르고 안정적

---

### 시나리오 3: "Alpine 이미지에서 에러가 나요!"

**상황**: 다른 서버에서 미리 빌드한 바이너리를 Alpine 이미지로 복사했는데 실행이 안 됩니다.

```dockerfile
# ❌ glibc 바이너리를 Alpine에서 실행
FROM alpine:3.18

WORKDIR /app

# Ubuntu에서 빌드한 바이너리 복사
COPY ./prebuilt-binary /usr/local/bin/myapp

CMD ["myapp"]
```

**에러 메시지**:
```bash
$ docker run myapp
exec /usr/local/bin/myapp: no such file or directory
# 또는
Error loading shared library libc.so.6: No such file or directory
```

**문제점**:
- 문제 1: Alpine은 musl libc 사용, Ubuntu는 glibc 사용
- 문제 2: 두 라이브러리는 호환되지 않음
- 문제 3: 바이너리가 glibc를 찾지 못해 실행 실패
- 왜 이 문제가 발생하는가: Alpine과 다른 배포판의 차이를 몰랐음

**해결책 1: Alpine에서 직접 빌드**:
```dockerfile
# ✅ Alpine에서 직접 빌드
FROM alpine:3.18 AS builder

RUN apk add --no-cache \
    gcc \
    musl-dev \
    make

WORKDIR /build
COPY src/ ./
RUN gcc -o myapp main.c

# 런타임
FROM alpine:3.18
COPY --from=builder /build/myapp /usr/local/bin/
CMD ["myapp"]
```

**해결책 2: glibc 호환 레이어 추가**:
```dockerfile
# ✅ glibc 호환성 추가 (권장하지 않음)
FROM alpine:3.18

RUN apk add --no-cache libc6-compat

COPY ./prebuilt-binary /usr/local/bin/myapp

CMD ["myapp"]
```

**해결책 3: Debian Slim 사용**:
```dockerfile
# ✅ glibc 기반 이미지 사용
FROM debian:bookworm-slim

COPY ./prebuilt-binary /usr/local/bin/myapp

CMD ["myapp"]
```

**배운 점**:
- 💡 팁 1: Alpine은 musl libc를 사용한다는 것을 기억하세요
- 💡 팁 2: 미리 빌드한 바이너리는 같은 libc 환경에서 실행해야 합니다
- 💡 팁 3: Alpine에서는 직접 빌드하는 것이 가장 안전합니다
- 💡 팁 4: glibc가 필요하면 Debian Slim을 고려하세요

---

### 시나리오 4: "보안 취약점이 50개나 나왔어요!"

**상황**: 코드 리뷰에서 "Trivy 스캔 먼저 하세요"라는 피드백을 받았고, 스캔 결과 취약점 50개가 발견되었습니다.

```dockerfile
# ❌ 오래된 베이스 이미지 사용
FROM node:16

WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .

CMD ["node", "server.js"]
```

**Trivy 스캔 결과**:
```bash
$ trivy image myapp:latest

myapp:latest (node 16.20.0)
============================
Total: 50 (UNKNOWN: 2, LOW: 15, MEDIUM: 20, HIGH: 10, CRITICAL: 3)

┌──────────────┬────────────────┬──────────┬───────────────┬──────────────┐
│   Library    │ Vulnerability  │ Severity │ Installed Ver │ Fixed Version│
├──────────────┼────────────────┼──────────┼───────────────┼──────────────┤
│ openssl      │ CVE-2023-12345 │ CRITICAL │ 3.0.8         │ 3.0.12       │
│ curl         │ CVE-2023-23456 │ HIGH     │ 7.88.0        │ 7.88.1       │
│ libxml2      │ CVE-2023-34567 │ HIGH     │ 2.9.14        │ 2.9.15       │
└──────────────┴────────────────┴──────────┴───────────────┴──────────────┘
```

**문제점**:
- 문제 1: Node.js 16은 이미 LTS 기간이 종료됨
- 문제 2: 전체 이미지 사용으로 불필요한 패키지가 많음
- 문제 3: 베이스 이미지에 오래된 라이브러리 포함
- 왜 이 문제가 발생하는가: 베이스 이미지 버전을 신경 쓰지 않았음

**해결책**:
```dockerfile
# ✅ 최신 LTS + Alpine + 멀티스테이지
FROM node:20-alpine AS deps

WORKDIR /deps
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# 빌드 단계
FROM node:20-alpine AS builder

WORKDIR /build
COPY package*.json ./
RUN npm ci && npm cache clean --force
COPY . .
RUN npm run build

# 런타임 - 최소 이미지
FROM node:20-alpine

# 보안: root가 아닌 사용자로 실행
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

# 프로덕션 의존성만
COPY --from=deps --chown=nodejs:nodejs /deps/node_modules ./node_modules

# 빌드 결과물만
COPY --from=builder --chown=nodejs:nodejs /build/dist ./dist
COPY --chown=nodejs:nodejs package*.json ./

USER nodejs
EXPOSE 3000

CMD ["node", "dist/server.js"]
```

**개선된 스캔 결과**:
```bash
$ trivy image myapp:latest

myapp:latest (node 20.11.0)
============================
Total: 2 (UNKNOWN: 0, LOW: 1, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

# 취약점: 50개 → 2개 (96% 감소!)
# CRITICAL: 3개 → 0개
# HIGH: 10개 → 0개
```

**추가 보안 조치**:
```dockerfile
# HEALTHCHECK 추가
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3000/health', (r) => { process.exit(r.statusCode === 200 ? 0 : 1) })"

# 읽기 전용 파일 시스템 (docker run 시)
# docker run --read-only --tmpfs /tmp myapp
```

**배운 점**:
- 💡 팁 1: 항상 최신 LTS 버전을 사용하세요 (Node 20, Python 3.11 등)
- 💡 팁 2: Alpine 이미지로 공격 표면을 최소화하세요
- 💡 팁 3: 멀티스테이지로 불필요한 도구를 제거하세요
- 💡 팁 4: root 대신 일반 사용자로 실행하세요
- 💡 팁 5: CI/CD에 Trivy 스캔을 통합하세요

---

## ❓ FAQ

<details>
<summary><strong>Q1: Alpine 이미지가 항상 최선인가요?</strong></summary>

**A**: 대부분의 경우 그렇지만, 항상 그런 것은 아닙니다.

**상세 설명**:
- Alpine의 장점: 작은 크기 (7MB), 빠른 다운로드, 적은 취약점
- Alpine의 단점: musl libc 호환성 문제, 일부 C 확장 빌드 어려움

**언제 Alpine을 사용하지 말아야 할까**:
- Python 프로젝트에서 numpy, pandas 등 C 확장이 많을 때
- 미리 빌드된 glibc 바이너리를 사용해야 할 때
- musl 관련 버그 트러블슈팅에 시간을 쓰고 싶지 않을 때

**대안**:
```dockerfile
# Python + 많은 C 확장 → Debian Slim 사용
FROM python:3.11-slim

# Node.js → Alpine 적합
FROM node:20-alpine

# Go 정적 바이너리 → Scratch 최고
FROM scratch
```

**실무 팁**:
💡 첫 프로젝트에서는 Alpine 시도 → 문제 발생 시 Debian Slim으로 전환

</details>

<details>
<summary><strong>Q2: 멀티스테이지 빌드는 언제 사용하나요?</strong></summary>

**A**: 빌드 도구와 런타임을 분리할 수 있는 모든 상황에서 사용합니다.

**상세 설명**:
- 컴파일 언어: Go, Rust, C/C++ (필수!)
- 스크립트 언어: Node.js, Python (권장!)
- 정적 사이트: React, Vue 등 프론트엔드 (필수!)

**효과**:
| 언어 | 단일 스테이지 | 멀티스테이지 | 감소율 |
|------|---------------|--------------|--------|
| Go | 800MB | 10MB | 99% |
| Node.js | 1.2GB | 50MB | 96% |
| React | 1.5GB | 25MB (nginx) | 98% |

**예시**:
```dockerfile
# React 프로젝트
FROM node:20-alpine AS builder
WORKDIR /build
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

# Nginx로 서빙
FROM nginx:alpine
COPY --from=builder /build/dist /usr/share/nginx/html
```

**실무 팁**:
💡 단일 스테이지는 개발 환경에서만, 프로덕션은 항상 멀티스테이지!

</details>

<details>
<summary><strong>Q3: .dockerignore를 꼭 만들어야 하나요?</strong></summary>

**A**: 네, 필수입니다! .gitignore만큼 중요합니다.

**상세 설명**:
- .dockerignore가 없으면 모든 파일이 빌드 컨텍스트에 포함됨
- node_modules, .git, 테스트 파일 등이 모두 Docker 데몬으로 전송됨
- 빌드 시간 증가, 이미지 크기 증가

**실제 차이**:
```bash
# .dockerignore 없을 때
$ docker build .
Sending build context to Docker daemon  1.2GB  ⏳⏳⏳
Step 1/10 : FROM node:20-alpine
...

# .dockerignore 있을 때
$ docker build .
Sending build context to Docker daemon  5.8MB  ⚡
Step 1/10 : FROM node:20-alpine
...
```

**필수 .dockerignore 템플릿**:
```
# 의존성
node_modules/
__pycache__/
venv/

# Git
.git/
.gitignore

# IDE
.vscode/
.idea/
*.swp

# 환경 변수
.env*

# 문서
*.md
docs/

# 테스트
coverage/
__tests__/
*.test.js
```

**실무 팁**:
💡 프로젝트 시작할 때 Dockerfile과 .dockerignore를 함께 만드세요!

</details>

<details>
<summary><strong>Q4: 이미지를 50MB 이하로 만들 수 있나요?</strong></summary>

**A**: 언어와 의존성에 따라 다르지만, 대부분 가능합니다.

**상세 설명**:
- Go: 10MB 이하 가능 (정적 바이너리 + scratch)
- Node.js: 50-100MB (Alpine + 멀티스테이지)
- Python: 80-150MB (Slim + 의존성 최소화)
- Java: 100-200MB (커스텀 JRE)

**극한의 최적화 (Go 예시)**:
```dockerfile
# 빌드
FROM golang:1.21-alpine AS builder
RUN apk add --no-cache upx
WORKDIR /build
COPY . .
RUN CGO_ENABLED=0 go build -ldflags="-w -s" -o app .
RUN upx --best --lzma app

# 런타임
FROM scratch
COPY --from=builder /build/app /
CMD ["/app"]
```

**결과**:
- 빌드 전: 15MB
- 최적화 후: 3MB (80% 감소!)

**주의사항**:
- UPX 압축은 실행 시 약간의 오버헤드 발생
- 디버깅이 어려워질 수 있음
- 프로덕션 배포 빈도가 높을 때만 권장

**실무 팁**:
💡 목표 크기: Go < 20MB, Node.js < 100MB, Python < 150MB

</details>

<details>
<summary><strong>Q5: 레이어 수를 줄이는 게 좋나요?</strong></summary>

**A**: 꼭 그런 것은 아닙니다. 캐싱 효율성이 더 중요합니다.

**상세 설명**:
- 예전 Docker: 레이어 수 제한 (127개) → 줄이는 게 중요
- 최신 Docker: 제한 없음 → 캐싱 효율이 더 중요

**❌ 나쁜 예 (무조건 레이어 줄이기)**:
```dockerfile
FROM node:20-alpine
WORKDIR /app
# 모든 것을 한 번에!
COPY . . && \
  npm ci && \
  npm run build && \
  npm cache clean --force
CMD ["node", "dist/server.js"]

# 문제: 소스가 바뀌면 npm ci도 재실행!
```

**✅ 좋은 예 (캐싱 최적화)**:
```dockerfile
FROM node:20-alpine
WORKDIR /app

# 레이어 1: 의존성 파일 (거의 변경 안 됨)
COPY package*.json ./

# 레이어 2: 의존성 설치 (캐시됨!)
RUN npm ci && npm cache clean --force

# 레이어 3: 소스 복사 (자주 변경됨)
COPY . .

# 레이어 4: 빌드
RUN npm run build

CMD ["node", "dist/server.js"]

# 장점: 소스 변경 시 레이어 1-2는 캐시 사용!
```

**실무 팁**:
💡 레이어 수보다 변경 빈도에 따른 순서가 더 중요합니다!

</details>

<details>
<summary><strong>Q6: BuildKit을 사용해야 하나요?</strong></summary>

**A**: 네! Docker 18.09 이상에서는 BuildKit이 기본이고, 더 빠릅니다.

**상세 설명**:
- BuildKit: Docker의 새로운 빌드 엔진
- 병렬 빌드, 더 나은 캐싱, 빌드 시크릿 지원

**활성화 방법**:
```bash
# 환경 변수로 활성화
export DOCKER_BUILDKIT=1
docker build .

# 영구 활성화 (daemon.json)
{
  "features": {
    "buildkit": true
  }
}
```

**성능 비교**:
```bash
# 기존 빌드
$ time docker build .
real    3m20s

# BuildKit
$ DOCKER_BUILDKIT=1 time docker build .
real    1m40s

# 50% 빠름!
```

**추가 기능**:
```dockerfile
# 빌드 시크릿 (BuildKit 전용)
RUN --mount=type=secret,id=npm_token \
    echo "//registry.npmjs.org/:_authToken=$(cat /run/secrets/npm_token)" > .npmrc && \
    npm ci && \
    rm .npmrc

# 빌드 캐시 마운트
RUN --mount=type=cache,target=/root/.npm \
    npm ci
```

**실무 팁**:
💡 Docker 18.09 이상이면 무조건 BuildKit 사용하세요!

</details>

<details>
<summary><strong>Q7: 이미지 최적화가 성능에도 영향을 주나요?</strong></summary>

**A**: 네! 배포 속도, 시작 시간, 메모리 사용량 모두 개선됩니다.

**상세 설명**:
- 이미지 크기 감소 → 다운로드 시간 감소 → 배포 속도 향상
- 불필요한 파일 제거 → 메모리 사용량 감소
- 최소 이미지 → 컨테이너 시작 시간 단축

**실제 성능 측정**:
```bash
# 1.2GB 이미지
$ time docker run myapp:before
Startup time: 8.5s
Memory: 512MB

# 50MB 이미지
$ time docker run myapp:after
Startup time: 2.1s  (75% 빠름!)
Memory: 128MB (75% 감소!)
```

**대규모 배포 시나리오**:
```
[100개 컨테이너 동시 배포]

1.2GB 이미지:
- 다운로드 시간: 5분 × 10서버 = 50분
- 네트워크 대역폭: 120GB
- 배포 완료: 1시간

50MB 이미지:
- 다운로드 시간: 10초 × 10서버 = 1분 40초
- 네트워크 대역폭: 5GB
- 배포 완료: 5분

결과: 12배 빠른 배포! 🚀
```

**Kubernetes 환경**:
- 작은 이미지 → 노드 간 빠른 스케줄링
- 적은 메모리 → 더 많은 Pod 실행 가능
- 빠른 시작 → 오토스케일링 응답 시간 단축

**실무 팁**:
💡 이미지 최적화 = 비용 절감 + 성능 향상의 일석이조!

</details>

<details>
<summary><strong>Q8: 멀티 아키텍처 이미지는 어떻게 만드나요?</strong></summary>

**A**: Docker Buildx를 사용하면 여러 아키텍처를 동시에 빌드할 수 있습니다.

**상세 설명**:
- 필요성: Apple Silicon (ARM64), Intel/AMD (AMD64), Raspberry Pi (ARM/v7)
- Buildx: Docker의 멀티 플랫폼 빌드 도구

**활성화**:
```bash
# Buildx 플러그인 설치 (Docker Desktop은 기본 포함)
docker buildx version

# 새 빌더 생성
docker buildx create --name multiarch --use
docker buildx inspect --bootstrap
```

**Dockerfile**:
```dockerfile
FROM --platform=$BUILDPLATFORM golang:1.21-alpine AS builder

ARG TARGETOS
ARG TARGETARCH

WORKDIR /build
COPY . .

# 타겟 플랫폼에 맞게 빌드
RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
    go build -o app .

FROM alpine:3.18
COPY --from=builder /build/app /
CMD ["/app"]
```

**빌드 및 푸시**:
```bash
# 멀티 아키텍처 빌드
docker buildx build \
  --platform linux/amd64,linux/arm64,linux/arm/v7 \
  -t myregistry.com/myapp:latest \
  --push \
  .

# 확인
docker buildx imagetools inspect myregistry.com/myapp:latest

# 출력:
# MediaType: application/vnd.docker.distribution.manifest.list.v2+json
# Platforms:
#   - linux/amd64
#   - linux/arm64
#   - linux/arm/v7
```

**실행** (자동으로 현재 플랫폼 선택):
```bash
# M1 Mac에서 실행 → ARM64 이미지 사용
docker run myregistry.com/myapp:latest

# Intel Mac에서 실행 → AMD64 이미지 사용
docker run myregistry.com/myapp:latest
```

**실무 팁**:
💡 공개 이미지나 다양한 환경 배포 시 멀티 아키텍처 필수!

</details>

---

## 💼 면접 질문 리스트

### 📘 주니어/신입 개발자용

<details>
<summary><strong>1. Docker 이미지 최적화가 왜 중요한가요?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 배포 속도 향상 (작은 이미지 = 빠른 다운로드)
- 포인트 2: 비용 절감 (스토리지, 네트워크 대역폭)
- 포인트 3: 보안 향상 (공격 표면 감소)
- 포인트 4: 성능 개선 (메모리 사용량, 시작 시간)

**예시 답변**
> "Docker 이미지 최적화는 여러 면에서 중요합니다. 첫째, 이미지 크기가 작을수록 다운로드 시간이 짧아져 배포 속도가 빨라집니다. 1.2GB 이미지를 50MB로 줄이면 배포 시간을 90% 이상 단축할 수 있습니다. 둘째, 클라우드 환경에서 스토리지와 네트워크 비용을 크게 절감할 수 있습니다. 셋째, 불필요한 패키지와 도구를 제거하면 보안 취약점을 줄일 수 있습니다. 마지막으로, 컨테이너 시작 시간과 메모리 사용량도 개선됩니다."

**꼬리 질문**
- Q: 이미지를 얼마나 작게 만들 수 있나요?
- A: 언어에 따라 다르지만, Go는 10MB 이하, Node.js는 50-100MB, Python은 80-150MB 정도가 일반적입니다. Go로 작성한 정적 바이너리는 scratch 이미지를 사용해 3-10MB까지 가능합니다.

**실무 연관**
- 대규모 마이크로서비스 환경에서 수백 개의 컨테이너를 배포할 때 이미지 크기가 배포 시간에 직접적인 영향을 줍니다.

</details>

<details>
<summary><strong>2. 멀티스테이지 빌드가 무엇이고 왜 사용하나요?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 빌드 단계와 런타임 단계를 분리하는 기법
- 포인트 2: 빌드 도구와 의존성을 최종 이미지에서 제거
- 포인트 3: 이미지 크기를 크게 줄일 수 있음 (70-99%)
- 포인트 4: FROM 명령어를 여러 번 사용

**예시 답변**
> "멀티스테이지 빌드는 Dockerfile에서 여러 개의 FROM 명령어를 사용해 빌드 과정을 여러 단계로 나누는 기법입니다. 첫 번째 단계에서는 컴파일러, 빌드 도구 등을 사용해 애플리케이션을 빌드하고, 두 번째 단계에서는 빌드된 결과물만 복사해서 최종 이미지를 만듭니다. 이렇게 하면 빌드 도구가 최종 이미지에 포함되지 않아 크기가 크게 줄어듭니다. 예를 들어 Go 애플리케이션의 경우 800MB에서 10MB로 줄일 수 있습니다."

**예시 코드**:
```dockerfile
# 빌드 단계
FROM golang:1.21 AS builder
WORKDIR /build
COPY . .
RUN go build -o app .

# 런타임 단계
FROM alpine:3.18
COPY --from=builder /build/app /
CMD ["/app"]
```

**꼬리 질문**
- Q: 스크립트 언어(Node.js, Python)에서도 멀티스테이지가 유용한가요?
- A: 네! devDependencies와 프로덕션 의존성을 분리하거나, 빌드 도구(webpack, babel)를 제거하는 데 유용합니다.

**실무 연관**
- 프론트엔드 React 프로젝트에서 node_modules와 빌드 도구를 제거하고 nginx로 서빙하면 1.5GB에서 25MB로 줄일 수 있습니다.

</details>

<details>
<summary><strong>3. Alpine Linux 베이스 이미지의 장단점은?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 매우 작은 크기 (7MB vs Ubuntu 72MB)
- 포인트 2: 보안 취약점이 적음 (패키지 수가 적음)
- 포인트 3: musl libc 사용으로 호환성 문제 가능
- 포인트 4: 일부 C 확장 빌드 시 추가 작업 필요

**예시 답변**
> "Alpine Linux의 가장 큰 장점은 크기입니다. 기본 이미지가 7MB 정도로 Ubuntu의 72MB보다 훨씬 작습니다. 또한 최소한의 패키지만 포함되어 있어 보안 취약점도 적습니다. 하지만 단점도 있습니다. Alpine은 musl libc를 사용하는데, 대부분의 리눅스는 glibc를 사용하기 때문에 호환성 문제가 발생할 수 있습니다. 특히 Python에서 numpy, pandas 같은 C 확장을 사용할 때 빌드가 복잡해질 수 있습니다."

**장단점 정리**:
```
장점:
✅ 매우 작은 크기 (7MB)
✅ 빠른 다운로드
✅ 적은 보안 취약점
✅ apk 패키지 관리자 (간단)

단점:
❌ musl libc 호환성 문제
❌ C 확장 빌드 복잡
❌ 일부 바이너리 실행 안 됨
❌ 디버깅 도구 부족
```

**꼬리 질문**
- Q: Alpine 대신 사용할 수 있는 대안은?
- A: Debian Slim (27MB, glibc 사용), Distroless (2MB, 셸 없음), Ubuntu Minimal 등이 있습니다.

**실무 연관**
- Node.js 프로젝트는 대부분 Alpine이 문제없지만, Python 프로젝트에서 과학 라이브러리를 사용한다면 Debian Slim을 고려해야 합니다.

</details>

<details>
<summary><strong>4. .dockerignore 파일이 왜 필요한가요?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 불필요한 파일을 빌드 컨텍스트에서 제외
- 포인트 2: 빌드 시간 단축 (Docker 데몬으로 전송량 감소)
- 포인트 3: 이미지 크기 감소
- 포인트 4: 보안 (환경 변수, 시크릿 제외)

**예시 답변**
> ".dockerignore 파일은 Docker 빌드 시 제외할 파일과 디렉토리를 지정합니다. .gitignore와 비슷한 역할을 합니다. 이 파일이 없으면 node_modules, .git, 테스트 파일 등 모든 파일이 Docker 데몬으로 전송되어 빌드 시간이 오래 걸리고 이미지 크기도 커집니다. 실제로 .dockerignore를 추가하면 빌드 컨텍스트 크기를 1.2GB에서 5MB로 줄일 수 있습니다. 또한 .env 파일이나 credentials 같은 민감한 정보가 이미지에 포함되는 것을 방지합니다."

**필수 항목**:
```
# .dockerignore
node_modules/
.git/
.env*
*.md
coverage/
__tests__/
.vscode/
```

**꼬리 질문**
- Q: COPY . . 명령어가 있으면 .dockerignore가 자동으로 적용되나요?
- A: 네, COPY와 ADD 명령어는 자동으로 .dockerignore를 참조합니다.

**실무 연관**
- CI/CD 파이프라인에서 빌드 시간이 오래 걸린다면 .dockerignore를 먼저 확인해보세요. 빌드 시간을 50% 이상 단축할 수 있습니다.

</details>

<details>
<summary><strong>5. Docker 레이어 캐싱을 어떻게 활용하나요?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 각 명령어(RUN, COPY 등)가 레이어를 생성
- 포인트 2: 레이어가 변경되지 않으면 캐시 사용
- 포인트 3: 자주 변경되는 것을 아래쪽에 배치
- 포인트 4: package.json 먼저 복사, 소스는 나중에

**예시 답변**
> "Docker는 Dockerfile의 각 명령어를 실행할 때마다 레이어를 생성하고 캐시합니다. 이전 빌드와 같은 명령어라면 캐시를 재사용해 빌드 시간을 크게 단축할 수 있습니다. 효율적으로 활용하려면 자주 변경되지 않는 것을 위에, 자주 변경되는 것을 아래에 배치해야 합니다. 예를 들어 Node.js에서는 package.json을 먼저 복사하고 npm install을 실행한 다음, 소스 코드를 복사합니다. 이렇게 하면 소스만 변경됐을 때 의존성 설치는 캐시를 사용하게 됩니다."

**좋은 예**:
```dockerfile
# ✅ 캐시 최적화
COPY package*.json ./
RUN npm ci  # 캐시됨!
COPY . .    # 자주 변경됨
```

**나쁜 예**:
```dockerfile
# ❌ 캐시 활용 못함
COPY . .       # 소스 변경 → 레이어 변경
RUN npm ci     # 매번 재실행!
```

**꼬리 질문**
- Q: 캐시를 무효화하고 싶을 때는?
- A: docker build --no-cache 옵션을 사용하거나, 특정 레이어 이후를 무효화하려면 ARG CACHEBUST=1 같은 변수를 사용할 수 있습니다.

**실무 연관**
- 대규모 모노레포에서 변경된 패키지만 다시 빌드하도록 레이어를 세밀하게 나누면 빌드 시간을 80% 이상 단축할 수 있습니다.

</details>

<details>
<summary><strong>6. RUN 명령어를 체이닝하는 이유는?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 여러 명령어를 && 로 연결해 하나의 레이어로 만듦
- 포인트 2: 레이어 수 감소보다 캐시 정리가 목적
- 포인트 3: 패키지 설치 후 캐시 정리를 같은 레이어에서
- 포인트 4: 임시 파일 정리도 같은 레이어에서

**예시 답변**
> "RUN 명령어를 체이닝하는 주된 이유는 임시 파일과 캐시를 같은 레이어에서 정리하기 위해서입니다. 예를 들어 apt-get update와 apt-get install을 별도 RUN으로 실행하면 캐시 파일이 레이어에 남아 이미지 크기가 커집니다. 하지만 && 로 연결하고 마지막에 rm -rf /var/lib/apt/lists/* 로 캐시를 정리하면 캐시가 최종 레이어에 포함되지 않습니다. 같은 원리로 wget으로 파일을 다운로드하고 압축 해제 후 원본 tar 파일을 삭제할 때도 체이닝을 사용합니다."

**예시**:
```dockerfile
# ❌ 캐시가 레이어에 남음
RUN apt-get update
RUN apt-get install -y nginx
RUN rm -rf /var/lib/apt/lists/*

# ✅ 캐시가 제거됨
RUN apt-get update && \
    apt-get install -y nginx && \
    rm -rf /var/lib/apt/lists/*
```

**크기 비교**:
- 별도 RUN: 135MB
- 체이닝: 85MB (37% 감소)

**꼬리 질문**
- Q: 모든 명령어를 하나로 체이닝하면 더 좋지 않나요?
- A: 아닙니다. 캐싱 효율이 떨어집니다. 논리적으로 관련된 명령어끼리만 체이닝하는 것이 좋습니다.

**실무 연관**
- Ubuntu/Debian 기반 이미지에서 apt 캐시를 정리하지 않으면 50MB 이상 낭비될 수 있습니다.

</details>

<details>
<summary><strong>7. 컨테이너를 root가 아닌 사용자로 실행해야 하는 이유는?</strong></summary>

**모범 답안 포인트**
- 포인트 1: 보안 원칙 - 최소 권한의 원칙
- 포인트 2: 컨테이너 탈출 시 피해 최소화
- 포인트 3: Kubernetes에서 권장하는 보안 설정
- 포인트 4: USER 명령어로 간단히 설정 가능

**예시 답변**
> "컨테이너를 root로 실행하면 보안 위험이 있습니다. 만약 애플리케이션에 취약점이 있어 컨테이너가 침해되면 공격자가 root 권한을 얻게 됩니다. 컨테이너 탈출 취약점이 발견되면 호스트 시스템까지 위험해질 수 있습니다. 따라서 최소 권한의 원칙에 따라 일반 사용자로 실행하는 것이 안전합니다. Kubernetes 같은 오케스트레이션 도구에서도 root 컨테이너를 제한하는 정책을 적용할 수 있습니다."

**구현 예시**:
```dockerfile
# 사용자 생성 (Alpine)
RUN addgroup -g 1001 -S appuser && \
    adduser -S appuser -u 1001 -G appuser

# 파일 소유권 변경
COPY --chown=appuser:appuser . .

# 사용자 전환
USER appuser

CMD ["node", "server.js"]
```

**Debian/Ubuntu**:
```dockerfile
RUN groupadd -r appuser && useradd -r -g appuser appuser
USER appuser
```

**꼬리 질문**
- Q: 포트 80을 사용해야 한다면?
- A: 1024 이하 포트는 root 권한이 필요합니다. 대신 3000 같은 높은 포트를 사용하고, 로드 밸런서나 리버스 프록시에서 80으로 매핑하세요.

**실무 연관**
- Kubernetes에서 securityContext.runAsNonRoot: true 설정을 하면 root 컨테이너가 실행을 거부당합니다.

</details>

---

### 📗 중급 개발자용

<details>
<summary><strong>1. BuildKit의 캐시 마운트 기능을 설명하고 활용 방법을 제시하세요</strong></summary>

**모범 답안 포인트**
- 심화 포인트 1: --mount=type=cache로 빌드 간 캐시 공유
- 심화 포인트 2: 패키지 관리자 캐시를 레이어 외부에 저장
- 심화 포인트 3: 멀티스테이지보다 빌드 속도 향상
- 내부 동작 원리: BuildKit이 호스트에 캐시 볼륨 생성

**예시 답변**
> "BuildKit의 캐시 마운트는 빌드 간에 공유되는 캐시 디렉토리를 만드는 기능입니다. 일반적으로 npm install이나 pip install을 실행하면 매번 패키지를 다운로드하지만, 캐시 마운트를 사용하면 이미 다운로드한 패키지를 재사용할 수 있습니다. --mount=type=cache,target=/root/.npm 같은 형태로 사용하며, 이 디렉토리는 레이어에 포함되지 않지만 다음 빌드에서 재사용됩니다."

**실무 예시**:
```dockerfile
# Node.js
RUN --mount=type=cache,target=/root/.npm \
    npm ci --only=production

# Python
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -r requirements.txt

# Go
RUN --mount=type=cache,target=/go/pkg/mod \
    go mod download
```

**성능 비교**:
```bash
# 일반 빌드 (캐시 없음)
[+] Building 180.0s
 => RUN npm ci          150.0s

# 캐시 마운트 (두 번째 빌드)
[+] Building 15.0s
 => RUN npm ci           10.0s  (93% 빠름!)
```

**꼬리 질문**
- Q: 캐시 마운트와 레이어 캐싱의 차이는?
- A: 레이어 캐싱은 명령어가 같을 때만 재사용되지만, 캐시 마운트는 명령어가 달라도 캐시 디렉토리를 공유합니다. 예를 들어 package.json이 변경돼도 이미 다운로드한 패키지는 재사용됩니다.

**실무 연관**
- CI/CD에서 캐시 마운트를 사용하면 빌드 시간을 50-70% 단축할 수 있습니다. GitHub Actions, GitLab CI에서 빌드 캐시 설정과 함께 사용하면 효과가 배가됩니다.

</details>

<details>
<summary><strong>2. Distroless 이미지와 Scratch 이미지의 차이와 사용 시나리오는?</strong></summary>

**모범 답안 포인트**
- 심화 포인트 1: Scratch는 완전히 빈 이미지, Distroless는 최소 런타임만
- 심화 포인트 2: Scratch는 정적 바이너리만, Distroless는 동적 링크 지원
- 심화 포인트 3: 디버깅 가능 여부의 차이
- 내부 동작 원리: 셸이 없어 docker exec 불가

**예시 답변**
> "Scratch는 Docker에서 제공하는 빈 이미지로, 0MB입니다. Go처럼 완전히 정적으로 링크된 바이너리만 실행할 수 있습니다. 반면 Distroless는 Google이 만든 이미지로, 런타임 라이브러리(libc, SSL 인증서 등)만 포함하고 셸, 패키지 관리자 등은 제거되어 있습니다. 크기는 2-20MB 정도이며 Java, Python, Node.js 같은 런타임이 필요한 언어에 적합합니다. 두 이미지 모두 셸이 없어 docker exec로 접속할 수 없지만, Distroless는 디버그 버전을 제공합니다."

**사용 시나리오**:
```dockerfile
# Scratch - Go 정적 바이너리
FROM golang:1.21 AS builder
RUN CGO_ENABLED=0 go build -o app .

FROM scratch
COPY --from=builder /app /
CMD ["/app"]

# Distroless - Java
FROM maven:3.9-openjdk-17 AS builder
RUN mvn clean package

FROM gcr.io/distroless/java17-debian11
COPY --from=builder /app/target/*.jar /app.jar
CMD ["app.jar"]

# Distroless Debug - 디버깅 필요 시
FROM gcr.io/distroless/java17-debian11:debug
# 이 버전은 busybox 셸 포함
```

**비교표**:
| 특징 | Scratch | Distroless | Alpine |
|------|---------|------------|--------|
| 크기 | 0MB | 2-20MB | 7MB |
| 셸 | ❌ | ❌ (debug는 ✅) | ✅ |
| 런타임 라이브러리 | ❌ | ✅ | ✅ |
| 정적 바이너리 | ✅ | ✅ | ✅ |
| 동적 바이너리 | ❌ | ✅ | ✅ |
| 디버깅 | 매우 어려움 | 어려움 | 쉬움 |

**꼬리 질문**
- Q: Distroless 이미지는 어떻게 디버깅하나요?
- A: :debug 태그 버전을 사용하거나, docker cp로 파일을 복사해서 분석하거나, ephemeral container를 사용합니다.

**실무 연관**
- 금융, 의료 등 보안이 중요한 산업에서 Distroless를 많이 사용합니다. 셸이 없어 공격자가 컨테이너 내부에서 명령어를 실행할 수 없기 때문입니다.

</details>

<details>
<summary><strong>3. 이미지 레이어 스캔 도구(Trivy, Snyk)의 동작 원리와 CI/CD 통합 방법은?</strong></summary>

**모범 답안 포인트**
- 심화 포인트 1: CVE 데이터베이스와 이미지 레이어 비교
- 심화 포인트 2: 패키지 매니페스트 파일 분석
- 심화 포인트 3: 빌드 파이프라인에서 자동 스캔 및 차단
- 내부 동작 원리: OCI 이미지 매니페스트 파싱 후 패키지 추출

**예시 답변**
> "Trivy와 Snyk 같은 도구는 Docker 이미지를 레이어별로 분석해 설치된 패키지를 추출하고, CVE(Common Vulnerabilities and Exposures) 데이터베이스와 비교합니다. Alpine의 경우 /lib/apk/db/installed를 읽고, Debian은 /var/lib/dpkg/status를 분석합니다. 발견된 취약점은 심각도(CRITICAL, HIGH, MEDIUM, LOW)로 분류됩니다. CI/CD에서는 빌드 후 자동으로 스캔하고, HIGH 이상 취약점이 발견되면 빌드를 실패시켜 배포를 차단할 수 있습니다."

**Trivy 설치 및 사용**:
```bash
# 설치
brew install trivy  # macOS
apt-get install trivy  # Ubuntu

# 이미지 스캔
trivy image myapp:latest

# 심각도 필터
trivy image --severity HIGH,CRITICAL myapp:latest

# JSON 출력
trivy image -f json -o results.json myapp:latest
```

**GitHub Actions 통합**:
```yaml
name: Security Scan

on: [push, pull_request]

jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build Image
        run: docker build -t myapp:${{ github.sha }} .

      - name: Run Trivy Scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: myapp:${{ github.sha }}
          severity: 'HIGH,CRITICAL'
          exit-code: '1'  # 취약점 발견 시 실패

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: trivy-results
          path: trivy-results.json
```

**GitLab CI 통합**:
```yaml
security-scan:
  stage: test
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - merge_requests
    - main
```

**꼬리 질문**
- Q: 취약점이 발견됐는데 패치가 없다면?
- A: 세 가지 옵션이 있습니다. 1) 다른 패키지로 대체, 2) .trivyignore 파일로 예외 처리 (정당한 사유 명시), 3) 워크어라운드 적용 후 모니터링

**실무 연관**
- 대부분의 조직은 프로덕션 배포 전 필수로 보안 스캔을 수행합니다. CRITICAL 취약점은 무조건 수정, HIGH는 비즈니스 영향 판단 후 결정하는 정책이 일반적입니다.

</details>

<details>
<summary><strong>4. 대규모 모노레포에서 Docker 빌드를 최적화하는 전략은?</strong></summary>

**모범 답안 포인트**
- 심화 포인트 1: 변경된 서비스만 선택적으로 빌드
- 심화 포인트 2: 공통 의존성을 베이스 이미지로 분리
- 심화 포인트 3: BuildKit의 빌드 캐시 원격 저장소 활용
- 내부 동작 원리: DOCKER_BUILDKIT=1 + --cache-from

**예시 답변**
> "모노레포에서는 여러 서비스가 하나의 저장소에 있기 때문에, 작은 변경에도 모든 서비스를 빌드하면 시간이 오래 걸립니다. 첫째, 변경 감지 도구로 수정된 서비스만 빌드합니다. 둘째, 공통 의존성을 별도 베이스 이미지로 만들어 재사용합니다. 셋째, BuildKit의 캐시 익스포트 기능으로 CI 서버 간 캐시를 공유합니다. 넷째, 멀티 아키텍처 빌드는 필요할 때만 수행합니다."

**프로젝트 구조**:
```
monorepo/
├── services/
│   ├── api/
│   ├── web/
│   └── worker/
├── packages/
│   └── shared/
└── docker/
    ├── base.Dockerfile
    └── node.Dockerfile
```

**베이스 이미지 전략**:
```dockerfile
# docker/base.Dockerfile
FROM node:20-alpine AS base
RUN apk add --no-cache dumb-init
WORKDIR /app

# 공통 의존성
FROM base AS deps
COPY packages/shared/package*.json ./packages/shared/
RUN cd packages/shared && npm ci
```

```dockerfile
# services/api/Dockerfile
FROM base AS api-deps
COPY services/api/package*.json ./
RUN npm ci

FROM base
COPY --from=deps /app/packages/shared /app/packages/shared
COPY --from=api-deps /app/node_modules /app/node_modules
COPY services/api /app
CMD ["dumb-init", "node", "server.js"]
```

**변경 감지 빌드 스크립트**:
```bash
#!/bin/bash
# build-changed.sh

CHANGED_SERVICES=$(git diff --name-only HEAD~1 | grep '^services/' | cut -d'/' -f2 | sort -u)

for service in $CHANGED_SERVICES; do
  echo "Building $service..."
  docker build -f services/$service/Dockerfile -t myregistry.com/$service:$GIT_SHA .
done
```

**원격 캐시 활용 (BuildKit)**:
```bash
# 캐시 익스포트
docker buildx build \
  --cache-to type=registry,ref=myregistry.com/cache:api \
  --cache-from type=registry,ref=myregistry.com/cache:api \
  -t myregistry.com/api:latest \
  --push \
  -f services/api/Dockerfile .

# 다른 CI 서버에서 캐시 재사용
docker buildx build \
  --cache-from type=registry,ref=myregistry.com/cache:api \
  -t myregistry.com/api:$GIT_SHA \
  -f services/api/Dockerfile .
```

**꼬리 질문**
- Q: Turborepo나 Nx 같은 빌드 도구와 어떻게 통합하나요?
- A: 이들 도구는 이미 변경 감지 기능을 제공하므로, turbo run build --filter=[changed] 결과를 Docker 빌드 트리거로 사용할 수 있습니다.

**실무 연관**
- Netflix, Uber 같은 대기업은 수백 개의 마이크로서비스를 모노레포에서 관리하며, 위 전략들을 조합해 빌드 시간을 몇 시간에서 10분 이하로 줄였습니다.

</details>

<details>
<summary><strong>5. UPX 압축과 스트립 최적화를 사용할 때의 트레이드오프는?</strong></summary>

**모범 답안 포인트**
- 심화 포인트 1: 이미지 크기 vs 실행 성능
- 심화 포인트 2: 디버깅 정보 제거의 장단점
- 심화 포인트 3: 메모리 사용량 증가
- 내부 동작 원리: 바이너리 압축 후 런타임 압축 해제

**예시 답변**
> "UPX는 실행 파일을 압축해 크기를 50-70% 줄이지만, 실행 시 메모리에 압축을 해제하는 오버헤드가 발생합니다. 일반적으로 0.1-0.3초 정도이며 메모리 사용량도 약간 증가합니다. 스트립 최적화(-ldflags='-w -s')는 디버그 심볼을 제거해 크기를 줄이지만, 스택 트레이스가 불완전해져 프로덕션 디버깅이 어려워집니다. 따라서 배포 빈도가 높고 네트워크 비용이 중요한 경우에만 권장합니다."

**최적화 단계별 비교**:
```dockerfile
# 1단계: 일반 빌드
FROM golang:1.21 AS builder
WORKDIR /build
COPY . .
RUN go build -o app .
# 크기: 15MB

# 2단계: 스트립 최적화
RUN CGO_ENABLED=0 go build -ldflags="-w -s" -o app .
# 크기: 8MB (47% 감소)
# 성능: 영향 없음
# 디버깅: 스택 트레이스 간소화

# 3단계: UPX 압축
FROM golang:1.21-alpine AS builder
RUN apk add --no-cache upx
WORKDIR /build
COPY . .
RUN CGO_ENABLED=0 go build -ldflags="-w -s" -o app .
RUN upx --best --lzma app
# 크기: 2.5MB (69% 추가 감소)
# 성능: 시작 시 0.1-0.3초 지연
# 메모리: 5-10MB 추가 사용
# 디버깅: 더 어려움
```

**성능 측정**:
```bash
# 일반 바이너리
$ time ./app
real    0m0.050s
user    0m0.030s
sys     0m0.015s

# UPX 압축 바이너리
$ time ./app
real    0m0.150s  (+0.1초)
user    0m0.080s
sys     0m0.020s
```

**메모리 사용량**:
```bash
# 일반 바이너리
$ docker stats myapp
CONTAINER   MEM USAGE
myapp       45MB

# UPX 압축 바이너리
$ docker stats myapp-upx
CONTAINER   MEM USAGE
myapp-upx   55MB (+10MB)
```

**사용 권장 사항**:
```
✅ UPX 사용 권장:
- 수백 개의 마이크로서비스 배포
- 엣지 컴퓨팅 (대역폭 제한)
- 빠른 스케일아웃 필요
- 시작 시간이 크리티컬하지 않음

❌ UPX 사용 비권장:
- 단일 서버 배포
- 실시간 성능 중요
- 메모리 최적화 우선
- 디버깅 빈도 높음
```

**꼬리 질문**
- Q: 안티바이러스가 UPX 압축 파일을 오탐지한다는데?
- A: 맞습니다. 일부 AV는 압축된 실행 파일을 의심합니다. 엔터프라이즈 환경에서는 화이트리스트에 추가하거나 코드 서명을 해야 할 수 있습니다.

**실무 연관**
- AWS Lambda나 Google Cloud Run처럼 배포 패키지 크기 제한이 있는 환경에서 UPX는 매우 유용합니다. 50MB 제한을 넘는 Go 바이너리를 20MB로 줄여 배포할 수 있습니다.

</details>

---

## 📝 핵심 정리

### 이 장에서 배운 핵심 개념

| 개념 | 설명 | 핵심 키워드 |
|------|------|-------------|
| 멀티스테이지 빌드 | 빌드 단계와 런타임 단계 분리 | COPY --from, AS builder |
| Alpine Linux | 경량 베이스 이미지 (7MB) | musl libc, apk |
| .dockerignore | 빌드 컨텍스트에서 파일 제외 | node_modules, .git |
| 레이어 캐싱 | 변경되지 않은 레이어 재사용 | 자주 변경되는 것 아래에 |
| RUN 체이닝 | 여러 명령어를 &&로 연결 | 캐시 정리 동시에 |
| 베이스 이미지 | 이미지의 시작점 | node:alpine, scratch |
| 보안 스캔 | 취약점 검사 도구 | Trivy, Snyk |
| BuildKit | 차세대 빌드 엔진 | 캐시 마운트, 병렬 빌드 |

### 필수 명령어/코드 정리

| 명령어/코드 | 용도 | 예시 |
|-------------|------|------|
| docker images | 이미지 목록 및 크기 확인 | docker images myapp |
| docker history | 레이어별 크기 분석 | docker history myapp:latest |
| dive | 이미지 레이어 탐색 | dive myapp:latest |
| trivy image | 보안 취약점 스캔 | trivy image myapp:latest |
| --mount=type=cache | BuildKit 캐시 마운트 | RUN --mount=type=cache,target=/root/.npm |
| COPY --from | 멀티스테이지에서 파일 복사 | COPY --from=builder /app/dist . |
| USER | 실행 사용자 변경 | USER nodejs |

### 실무 베스트 프랙티스

#### ✅ 해야 할 것
- [ ] 항상 Alpine 또는 Slim 이미지 우선 고려
- [ ] 멀티스테이지 빌드로 빌드 도구 제거
- [ ] .dockerignore 파일 필수 작성
- [ ] 의존성 파일 먼저 복사 후 설치 (캐싱)
- [ ] 패키지 관리자 캐시 정리 (--no-cache, --no-cache-dir)
- [ ] root가 아닌 사용자로 실행
- [ ] CI/CD에 보안 스캔 통합
- [ ] 명확한 버전 태그 사용 (latest 지양)

#### ❌ 하지 말아야 할 것
- [ ] 전체 베이스 이미지 사용 (node:18 대신 node:18-alpine)
- [ ] COPY . .을 의존성 설치 전에 사용
- [ ] .dockerignore 없이 빌드
- [ ] 별도 RUN 명령어로 캐시 정리
- [ ] 환경 변수나 시크릿을 이미지에 포함
- [ ] root 사용자로 실행
- [ ] 오래된 베이스 이미지 사용
- [ ] 불필요한 devDependencies 포함

### 성능/보안 체크리스트

#### 성능
- [ ] 이미지 크기 100MB 이하 (언어별 목표 다름)
- [ ] 빌드 시간 5분 이하 (캐시 활용 시 30초 이하)
- [ ] 레이어 수 15개 이하
- [ ] BuildKit 활성화
- [ ] 캐시 마운트 활용 (npm, pip 등)
- [ ] 멀티 아키텍처 빌드 (필요 시)

#### 보안
- [ ] 최신 LTS 베이스 이미지 사용
- [ ] HIGH/CRITICAL 취약점 0개
- [ ] root 사용자 비활성화
- [ ] 읽기 전용 파일 시스템 고려
- [ ] HEALTHCHECK 추가
- [ ] 민감 정보 제외 (.env, credentials)
- [ ] 빌드 시크릿 활용 (ARG 대신)

---

## 🔗 관련 기술

**이 기술과 함께 사용하는 기술들**

| 기술 | 관계 | 학습 우선순위 |
|------|------|---------------|
| Docker Compose | 멀티 컨테이너 최적화 | ⭐⭐⭐⭐ |
| Kubernetes | 프로덕션 배포 및 스케일링 | ⭐⭐⭐⭐⭐ |
| CI/CD (GitHub Actions, GitLab CI) | 자동화된 빌드 및 스캔 | ⭐⭐⭐⭐⭐ |
| Container Registry | 이미지 저장 및 배포 | ⭐⭐⭐⭐ |
| Trivy/Snyk | 보안 취약점 스캔 | ⭐⭐⭐⭐ |
| BuildKit | 고급 빌드 기능 | ⭐⭐⭐ |
| Dive | 레이어 분석 도구 | ⭐⭐ |

---

## 9.10 다음 단계

### 다음 장 미리보기: 섹션 10 - Docker 네트워크
- **배울 내용 1**: 컨테이너 간 통신 방법 (브리지, 호스트, 오버레이)
- **배울 내용 2**: 사용자 정의 네트워크 생성 및 관리
- **배울 내용 3**: 서비스 디스커버리와 DNS
- **배울 내용 4**: 네트워크 격리 및 보안
- **실전 프로젝트**: 마이크로서비스 네트워크 구성

### 이 장과의 연결점
```
이번 장: 이미지 최적화 (크기, 속도, 보안)
    ↓
다음 장: Docker 네트워크 (컨테이너 간 통신)
    ↓
그 다음: Docker 볼륨 (데이터 영속성)
    ↓
최종적으로: 프로덕션 환경 구축
```

### 준비하면 좋을 것들
```bash
# 다음 장 실습을 위한 준비
# 여러 컨테이너를 띄워볼 예정이므로 최적화된 이미지로!
docker build -t myapi:optimized .
docker build -t myweb:optimized .

# 네트워크 관련 도구 설치
docker pull nginx:alpine
docker pull redis:alpine
```

---

## 🎉 축하합니다!

**이제 여러분은**:
✅ Docker 이미지를 90% 이상 줄일 수 있습니다 (1.2GB → 50MB)
✅ 멀티스테이지 빌드로 빌드 도구를 완전히 제거할 수 있습니다
✅ 레이어 캐싱을 활용해 빌드 시간을 80% 단축할 수 있습니다
✅ Alpine, Distroless, Scratch 이미지의 차이를 이해하고 선택할 수 있습니다
✅ .dockerignore로 불필요한 파일을 제외할 수 있습니다
✅ 보안 스캔 도구로 취약점을 찾고 수정할 수 있습니다
✅ 프로덕션 수준의 최적화된 Dockerfile을 작성할 수 있습니다

**다음 단계**:
- [ ] 다음 장으로 진행: [10. Docker 네트워크](10-네트워크.md)
- [ ] 실전 프로젝트 도전: 기존 프로젝트의 이미지를 최적화해보세요
- [ ] 면접 질문 복습: 이 장의 면접 질문들을 다시 읽어보세요
- [ ] 보안 스캔 실습: 회사 프로젝트에 Trivy를 적용해보세요

---

**다음 장으로 이동**: [다음: 10. Docker 네트워크 →](10-네트워크.md)

**이전 장으로 돌아가기**: [← 이전: 08. 멀티스테이지 빌드](08-멀티스테이지-빌드.md)

**목차로 돌아가기**: [📚 전체 목차](README.md)