# 17장: 모니터링 및 로깅

> **학습 목표**: Docker 환경의 모니터링과 로깅 시스템을 구축하고, Prometheus + Grafana로 메트릭을 수집하며, ELK 스택으로 로그를 분석할 수 있습니다.

**⏱️ 예상 학습 시간**: 3-4시간
**난이도**: ⭐⭐⭐⭐☆ (4개/5개)

---

## 📚 목차
- [왜 모니터링과 로깅이 필요한가](#왜-모니터링과-로깅이-필요한가)
- [실생활 비유로 이해하기](#실생활-비유로-이해하기)
- [핵심 개념](#핵심-개념)
- [Prometheus + Grafana 설정](#prometheus--grafana-설정)
- [ELK 스택 로깅](#elk-스택-로깅)
- [실행 및 사용](#실행-및-사용)
- [주니어 시나리오](#주니어-시나리오)
- [FAQ](#faq)
- [면접 질문 리스트](#면접-질문-리스트)
- [핵심 정리](#핵심-정리)

---

## 🤔 왜 모니터링과 로깅이 필요한가?

### 실무 배경
**대규모 마이크로서비스 환경에서는 수십 개의 컨테이너가 동시에 실행됩니다. 어느 컨테이너에서 장애가 발생했는지, 어떤 서비스가 느려졌는지 실시간으로 파악하지 못하면 사용자에게 직접적인 피해가 발생합니다.**

#### ❌ 모니터링/로깅이 없으면 발생하는 문제
```
문제 1: 장애 발견 지연
- 증상: 사용자가 불만을 제기한 후에야 장애 인지
- 영향: 평균 장애 인지 시간 30분 이상
- 비용: 시간당 매출 손실 $10,000

문제 2: 원인 분석 불가
- 증상: 어느 서비스에서 문제가 발생했는지 모름
- 영향: 장애 복구 시간 2-4시간
- 비용: 개발자 3명이 4시간 동안 원인 분석

문제 3: 예측 불가능한 확장
- 증상: 갑작스러운 트래픽 증가에 대응 못함
- 영향: 서버 다운, 서비스 중단
- 비용: 고객 이탈, 브랜드 신뢰도 하락
```

#### ✅ 모니터링/로깅을 사용하면
```
해결책 1: 실시간 장애 감지
- 방법: Prometheus 알림으로 1분 이내 감지
- 효과: 장애 인지 시간 1분 이내
- 절감: 매출 손실 97% 감소

해결책 2: 빠른 원인 분석
- 방법: Grafana 대시보드와 Kibana 로그 분석
- 효과: 장애 복구 시간 15분 이내
- 절감: 개발자 시간 75% 절감

해결책 3: 예측 가능한 확장
- 방법: 메트릭 트렌드 분석으로 사전 스케일링
- 효과: 서비스 안정성 99.9% 유지
- 절감: 긴급 대응 비용 80% 감소
```

### 📊 수치로 보는 효과

| 지표 | Before | After | 개선율 |
|------|--------|-------|--------|
| 장애 인지 시간 | 30분 | 1분 | **97%↓** |
| 장애 복구 시간 | 4시간 | 15분 | **94%↓** |
| 로그 검색 시간 | 1시간 | 10초 | **99%↓** |
| 서비스 가용성 | 99% | 99.9% | **0.9%p↑** |
| 운영 비용 | $5,000/월 | $1,500/월 | **70%↓** |

---

## 🌟 실생활 비유로 이해하기

### 비유 1: 자동차 계기판 (모니터링)
```
Docker 모니터링 = 자동차 계기판

운전 중 확인하는 것:
- 속도계 (CPU 사용률) = 현재 얼마나 빠르게 달리는가?
- 연료 게이지 (메모리) = 얼마나 남았는가?
- 엔진 온도 (시스템 온도) = 과열되지 않았는가?
- 경고등 (알림) = 문제가 발생했는가?

┌─────────────────────────────────┐
│  🚗 자동차 계기판                │
│  ┌─────┐ ┌─────┐ ┌─────┐        │
│  │ 80  │ │ 🔋  │ │ 🌡️  │        │
│  │km/h │ │ 60% │ │ 90° │        │
│  └─────┘ └─────┘ └─────┘        │
│  ⚠️ 경고등: 점검 필요             │
└─────────────────────────────────┘

모니터링 없이 운전 = 계기판 없이 운전
→ 연료가 떨어질 때까지 모름
→ 엔진이 과열되어도 모름
→ 갑작스러운 고장 발생
```

### 비유 2: 병원 환자 모니터링 (알림)
**중환자실의 생체 신호 모니터**

모니터링 시스템이 하는 일:
- 심박수 → 초당 요청 수 (RPS)
- 혈압 → 응답 시간 (Latency)
- 산소 포화도 → 에러율
- 체온 → CPU/메모리 사용률

**경고 시나리오**:
- 정상 범위 이탈 → 삐삐삐 경고음 → 즉시 대응
- Docker 알림 → Slack 메시지 → 개발자 대응

### 비유 3: 비행기 블랙박스 (로깅)
**사고 발생 시 원인 분석**

```
비행기 블랙박스 = Docker 로그

기록하는 정보:
- 고도, 속도, 방향 = API 요청, 응답, 시간
- 조종사 대화 = 서비스 간 통신
- 시스템 경고 = 에러 메시지
- 기상 조건 = 네트워크 상태

사고 발생 → 블랙박스 분석 → 원인 파악
장애 발생 → 로그 분석 → 원인 파악

블랙박스 없으면?
→ 원인 파악 불가
→ 재발 방지 불가
→ 책임 소재 불분명
```

### 비유 4: CCTV 시스템 (로그 보관)
**보안 및 사후 분석**

```
CCTV = ELK 로그 시스템

실시간 모니터링:
- 여러 화면 동시 관찰 = Kibana 대시보드
- 특정 시간대 재생 = 로그 시간대 필터링
- 특정 인물 추적 = 특정 사용자 로그 추적
- 이상 행동 감지 = 에러 패턴 감지

┌─────────────────────────────────┐
│  📹 CCTV 관제 센터               │
│  ┌─────┐ ┌─────┐ ┌─────┐        │
│  │화면1│ │화면2│ │화면3│        │
│  │로비 │ │주차장│ │입구 │        │
│  └─────┘ └─────┘ └─────┘        │
│  🔴 녹화 중 | 📅 30일 보관      │
└─────────────────────────────────┘

사고 발생 → CCTV 확인 → 범인 추적
장애 발생 → 로그 확인 → 원인 추적
```

### 비유 5: 기상청 데이터 수집 (메트릭 수집)
**전국 관측소의 데이터를 중앙에서 취합**

```
기상청 시스템 = Prometheus + Grafana

관측소 (Exporter):
- 서울 기상대 → user-service
- 부산 기상대 → account-service
- 제주 기상대 → trade-service

수집하는 데이터:
- 온도, 습도, 풍속 → CPU, 메모리, 디스크
- 15분마다 수집 → 15초마다 스크래핑
- 중앙 서버에 저장 → Prometheus 저장소

데이터 활용:
- 기상 예보 → 리소스 예측
- 이상 기후 감지 → 장애 감지
- 날씨 지도 → Grafana 대시보드
```

### 🎯 종합 비교표
```
┌──────────────┬──────────────┬──────────────┬──────────────┐
│ 기술         │ 실생활 비유   │ 주요 기능     │ 핵심 가치     │
├──────────────┼──────────────┼──────────────┼──────────────┤
│ Prometheus   │ 기상청       │ 메트릭 수집   │ 실시간 감시   │
│ Grafana      │ 계기판       │ 시각화       │ 직관적 이해   │
│ Alertmanager │ 경고등       │ 알림 발송     │ 즉시 대응     │
│ ELK Stack    │ CCTV+블랙박스│ 로그 분석     │ 사후 분석     │
└──────────────┴──────────────┴──────────────┴──────────────┘
```

---

## 📖 핵심 개념

### 개념 설명 (3단계 깊이)

#### 1️⃣ 초보자 수준 설명
**모니터링은 시스템이 잘 돌아가는지 실시간으로 확인하는 것이고, 로깅은 무슨 일이 있었는지 기록해두는 것입니다.**

모니터링 = 지금 상태 확인
로깅 = 과거 기록 확인

#### 2️⃣ 중급자 수준 설명
**모니터링은 메트릭(숫자 데이터)을 주기적으로 수집하여 시스템 상태를 추적하고, 임계값을 초과하면 알림을 보냅니다. 로깅은 애플리케이션과 시스템의 이벤트를 텍스트로 기록하여 문제 발생 시 원인을 분석합니다.**

Prometheus가 메트릭을 수집하면 → Grafana가 시각화하고 → Alertmanager가 알림을 보냅니다.

#### 3️⃣ 고급자 수준 설명
**Prometheus는 Pull 방식의 시계열 데이터베이스로, HTTP 엔드포인트를 스크래핑하여 메트릭을 수집하고 PromQL로 쿼리합니다. ELK는 Filebeat가 로그를 수집하여 Logstash가 파싱/변환하고, Elasticsearch에 인덱싱한 후 Kibana로 검색/시각화하는 파이프라인입니다.**

### 4가지 골든 시그널

```
1. Latency (지연 시간)
   - 요청 처리 시간
   - 목표: API 응답 < 200ms
   - 측정: histogram_quantile(0.95, ...)

2. Traffic (트래픽)
   - 초당 요청 수 (RPS)
   - 목표: 1000 RPS 처리
   - 측정: rate(http_requests_total[5m])

3. Errors (에러율)
   - 실패한 요청 비율
   - 목표: 에러율 < 0.1%
   - 측정: rate(http_errors[5m]) / rate(http_requests[5m])

4. Saturation (포화도)
   - 리소스 사용률
   - 목표: CPU < 70%, 메모리 < 80%
   - 측정: container_cpu_usage / container_cpu_limit
```

### 주요 용어 정리

| 용어 | 영문 | 설명 | 예시 |
|------|------|------|------|
| 메트릭 | Metric | 숫자로 표현되는 측정값 | CPU 사용률 75% |
| 스크래핑 | Scraping | 주기적으로 데이터 수집 | 15초마다 /metrics 호출 |
| 시계열 | Time Series | 시간에 따른 데이터 변화 | 시간별 메모리 사용량 |
| 프로메테우스 쿼리 | PromQL | 메트릭 조회 언어 | `rate(requests[5m])` |
| 익스포터 | Exporter | 메트릭을 노출하는 에이전트 | node-exporter, postgres-exporter |
| 알림 규칙 | Alert Rule | 알림 발송 조건 | CPU > 80% 5분 지속 시 |
| 인덱싱 | Indexing | 로그를 검색 가능하게 저장 | Elasticsearch 인덱스 |
| 파이프라인 | Pipeline | 로그 처리 흐름 | 수집→파싱→저장→검색 |

### 기술 아키텍처

```
┌─────────────────────────────────────────────────────────────┐
│                   LK-Trade 모니터링 및 로깅 시스템            │
└─────────────────────────────────────────────────────────────┘

[모니터링 스택 - Prometheus + Grafana]

┌──────────────┐
│   Grafana    │ ← 사용자가 대시보드로 확인
│  (시각화)     │
└──────┬───────┘
       │ PromQL 쿼리
┌──────▼────────────────────────────────┐
│        Prometheus                     │
│  (메트릭 수집, 저장, 쿼리)             │
└──┬────┬────┬────┬────┬────┬──────────┘
   │    │    │    │    │    │
   │    │    │    │    │    └─→ [cAdvisor] 컨테이너 메트릭
   │    │    │    │    └──────→ [Node Exp] 호스트 메트릭
   │    │    │    └───────────→ [PG Exp] PostgreSQL 메트릭
   │    │    └────────────────→ [Redis Exp] Redis 메트릭
   │    └─────────────────────→ [trade-svc] Spring Actuator
   └──────────────────────────→ [user-svc] Spring Actuator

[로깅 스택 - ELK]

┌──────────────┐
│    Kibana    │ ← 사용자가 로그 검색/분석
│  (검색/시각화)│
└──────┬───────┘
       │
┌──────▼──────────────────────┐
│     Elasticsearch           │
│  (로그 저장, 인덱싱, 검색)   │
└──────▲──────────────────────┘
       │
┌──────┴──────────────────────┐
│       Logstash              │
│  (로그 파싱, 변환, 필터링)   │
└──────▲──────────────────────┘
       │
┌──────┴──────────────────────┐
│       Filebeat              │
│  (로그 수집, 전송)           │
└──┬────┬────┬────┬───────────┘
   │    │    │    │
   ▼    ▼    ▼    ▼
[user] [acc] [trd] [nginx]
서비스 로그 파일들

설명:
- Prometheus: 15초마다 각 서비스의 /metrics 엔드포인트를 스크래핑
- Grafana: Prometheus 데이터를 그래프로 시각화
- Filebeat: 컨테이너 로그를 실시간 수집
- Logstash: JSON 파싱 및 필터링
- Elasticsearch: 인덱싱된 로그 저장
- Kibana: 로그 검색 및 대시보드
```

---

## 17.2 Prometheus + Grafana 설정

### 17.2.1 아키텍처

```
[LK-Trade 모니터링 스택]

┌─────────────────────────────────────────────┐
│              Grafana Dashboard              │
│  (시각화, 대시보드, 알림)                    │
└────────────────┬────────────────────────────┘
                 │ 쿼리 (PromQL)
┌────────────────▼────────────────────────────┐
│              Prometheus                     │
│  (메트릭 수집, 저장, 쿼리)                   │
└─┬──────┬──────┬──────┬──────┬──────┬───────┘
  │      │      │      │      │      │
  │      │      │      │      │      │
  ▼      ▼      ▼      ▼      ▼      ▼
[User] [Acc] [Trd] [AI] [Scr] [Nginx]
Service Service Service Service Service Exporter

각 서비스: /actuator/prometheus 엔드포인트
```

---

### 17.2.2 Spring Boot Actuator 설정

**modules/user/api/build.gradle.kts:**

```kotlin
dependencies {
    // Spring Boot Actuator
    implementation("org.springframework.boot:spring-boot-starter-actuator")

    // Micrometer Prometheus
    implementation("io.micrometer:micrometer-registry-prometheus")
}
```

**modules/user/api/src/main/resources/application.yml:**

```yaml
management:
  endpoints:
    web:
      exposure:
        include: health,info,prometheus,metrics
  endpoint:
    health:
      show-details: always
    prometheus:
      enabled: true
  metrics:
    export:
      prometheus:
        enabled: true
    distribution:
      percentiles-histogram:
        http.server.requests: true
    tags:
      application: ${spring.application.name}
      service: user-service
      environment: ${SPRING_PROFILES_ACTIVE:dev}
```

---

### 17.2.3 Prometheus 설정

**docker/prometheus/prometheus.yml:**

```yaml
# Prometheus 전역 설정
global:
  scrape_interval: 15s      # 15초마다 메트릭 수집
  evaluation_interval: 15s  # 15초마다 규칙 평가
  external_labels:
    cluster: 'lktrade'
    environment: 'dev'

# 알림 규칙 파일
rule_files:
  - '/etc/prometheus/rules/*.yml'

# Alertmanager 설정
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# 메트릭 수집 대상
scrape_configs:
  # Prometheus 자체 모니터링
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Spring Boot 서비스들
  - job_name: 'user-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['user-service:8081']
        labels:
          service: 'user'
          tier: 'application'

  - job_name: 'account-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['account-service:8082']
        labels:
          service: 'account'
          tier: 'application'

  - job_name: 'trade-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['trade-service:8083']
        labels:
          service: 'trade'
          tier: 'application'

  - job_name: 'ai-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['ai-service:8084']
        labels:
          service: 'ai'
          tier: 'application'

  - job_name: 'scraper-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['scraper-service:8085']
        labels:
          service: 'scraper'
          tier: 'application'

  # Nginx Exporter
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
        labels:
          service: 'nginx'
          tier: 'gateway'

  # PostgreSQL Exporter
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
        labels:
          service: 'postgres'
          tier: 'database'

  # Redis Exporter
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
        labels:
          service: 'redis'
          tier: 'cache'

  # cAdvisor (컨테이너 메트릭)
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
        labels:
          service: 'cadvisor'
          tier: 'monitoring'

  # Node Exporter (호스트 메트릭)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
        labels:
          service: 'node'
          tier: 'host'
```

---

### 17.2.4 알림 규칙

**docker/prometheus/rules/alerts.yml:**

```yaml
groups:
  # 서비스 가용성
  - name: service_availability
    interval: 30s
    rules:
      # 서비스 다운
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "서비스 다운: {{ $labels.job }}"
          description: "{{ $labels.instance }}가 1분 이상 응답하지 않습니다."

      # 높은 에러율
      - alert: HighErrorRate
        expr: |
          rate(http_server_requests_seconds_count{status=~"5.."}[5m])
          /
          rate(http_server_requests_seconds_count[5m])
          > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "높은 에러율: {{ $labels.service }}"
          description: "{{ $labels.service }}의 5xx 에러율이 5%를 초과했습니다."

  # 성능
  - name: performance
    interval: 30s
    rules:
      # 높은 응답 시간
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            rate(http_server_requests_seconds_bucket[5m])
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "높은 레이턴시: {{ $labels.service }}"
          description: "{{ $labels.service }}의 P95 응답 시간이 1초를 초과했습니다."

      # 높은 트래픽
      - alert: HighTraffic
        expr: |
          rate(http_server_requests_seconds_count[5m]) > 100
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "높은 트래픽: {{ $labels.service }}"
          description: "{{ $labels.service }}의 RPS가 100을 초과했습니다."

  # 리소스
  - name: resource_usage
    interval: 30s
    rules:
      # 높은 CPU 사용률
      - alert: HighCpuUsage
        expr: |
          (1 - avg by(container_label_com_docker_compose_service)
            (rate(container_cpu_usage_seconds_total[5m])))
          * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "높은 CPU 사용률: {{ $labels.container_label_com_docker_compose_service }}"
          description: "CPU 사용률이 80%를 초과했습니다."

      # 높은 메모리 사용률
      - alert: HighMemoryUsage
        expr: |
          (container_memory_usage_bytes
          / container_spec_memory_limit_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "높은 메모리 사용률: {{ $labels.container_label_com_docker_compose_service }}"
          description: "메모리 사용률이 80%를 초과했습니다."

      # 디스크 공간 부족
      - alert: LowDiskSpace
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"}
          / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "디스크 공간 부족"
          description: "루트 파티션의 여유 공간이 10% 미만입니다."

  # 데이터베이스
  - name: database
    interval: 30s
    rules:
      # PostgreSQL 연결 수
      - alert: PostgreSQLTooManyConnections
        expr: |
          sum(pg_stat_activity_count) > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL 연결 수 초과"
          description: "활성 연결이 90개를 초과했습니다."

      # Redis 메모리 부족
      - alert: RedisMemoryHigh
        expr: |
          (redis_memory_used_bytes
          / redis_memory_max_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis 메모리 부족"
          description: "Redis 메모리 사용률이 90%를 초과했습니다."
```

---

### 17.2.5 docker-compose.monitoring.yml

```yaml
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: lktrade-prometheus
    profiles: ["monitoring"]
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./docker/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - lktrade-network
    restart: unless-stopped

  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: lktrade-grafana
    profiles: ["monitoring"]
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: http://localhost:3001
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    ports:
      - "3001:3000"
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    networks:
      - lktrade-network
    depends_on:
      - prometheus
    restart: unless-stopped

  # Alertmanager
  alertmanager:
    image: prom/alertmanager:latest
    container_name: lktrade-alertmanager
    profiles: ["monitoring"]
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    ports:
      - "9093:9093"
    volumes:
      - ./docker/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    networks:
      - lktrade-network
    restart: unless-stopped

  # cAdvisor (컨테이너 메트릭)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: lktrade-cadvisor
    profiles: ["monitoring"]
    privileged: true
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    ports:
      - "8087:8080"
    networks:
      - lktrade-network
    restart: unless-stopped

  # Node Exporter (호스트 메트릭)
  node-exporter:
    image: prom/node-exporter:latest
    container_name: lktrade-node-exporter
    profiles: ["monitoring"]
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"
    networks:
      - lktrade-network
    restart: unless-stopped

  # PostgreSQL Exporter
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: lktrade-postgres-exporter
    profiles: ["monitoring"]
    environment:
      DATA_SOURCE_NAME: "postgresql://postgres:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/lktrade?sslmode=disable"
    ports:
      - "9187:9187"
    networks:
      - lktrade-network
    depends_on:
      - postgres
    restart: unless-stopped

  # Redis Exporter
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: lktrade-redis-exporter
    profiles: ["monitoring"]
    environment:
      REDIS_ADDR: redis:6379
    ports:
      - "9121:9121"
    networks:
      - lktrade-network
    depends_on:
      - redis
    restart: unless-stopped

  # Nginx Exporter
  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:latest
    container_name: lktrade-nginx-exporter
    profiles: ["monitoring"]
    command:
      - -nginx.scrape-uri=http://nginx:80/nginx_status
    ports:
      - "9113:9113"
    networks:
      - lktrade-network
    depends_on:
      - nginx
    restart: unless-stopped

volumes:
  prometheus-data:
    name: lktrade-prometheus-data
  grafana-data:
    name: lktrade-grafana-data
  alertmanager-data:
    name: lktrade-alertmanager-data
```

---

### 17.2.6 Grafana 대시보드 프로비저닝

**docker/grafana/provisioning/datasources/prometheus.yml:**

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

**docker/grafana/provisioning/dashboards/default.yml:**

```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
```

---

### 17.2.7 Alertmanager 설정

**docker/alertmanager/alertmanager.yml:**

```yaml
global:
  resolve_timeout: 5m
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# 알림 라우팅
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # Critical 알림은 즉시
    - match:
        severity: critical
      receiver: 'critical'
      group_wait: 0s
      repeat_interval: 5m

    # Warning 알림
    - match:
        severity: warning
      receiver: 'warning'
      repeat_interval: 30m

    # Info 알림
    - match:
        severity: info
      receiver: 'info'
      repeat_interval: 4h

# 알림 수신자
receivers:
  - name: 'default'
    slack_configs:
      - channel: '#lktrade-alerts'
        title: 'LK-Trade Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

  - name: 'critical'
    slack_configs:
      - channel: '#lktrade-critical'
        title: '🚨 Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'warning'
    slack_configs:
      - channel: '#lktrade-warnings'
        title: '⚠️  Warning Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

  - name: 'info'
    slack_configs:
      - channel: '#lktrade-info'
        title: 'ℹ️  Info Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

# 알림 억제 (중복 방지)
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
```

---

## 17.3 로깅 (ELK 스택)

### 17.3.1 ELK 스택 아키텍처

```
[LK-Trade 로깅 스택]

┌─────────────────────────────────────────────┐
│              Kibana                         │
│  (로그 검색, 시각화, 대시보드)               │
└────────────────┬────────────────────────────┘
                 │
┌────────────────▼────────────────────────────┐
│           Elasticsearch                     │
│  (로그 저장, 인덱싱, 검색)                   │
└─────────────────▲───────────────────────────┘
                  │
┌─────────────────┴───────────────────────────┐
│              Logstash                       │
│  (로그 수집, 파싱, 변환)                     │
└─┬──────┬──────┬──────┬──────┬──────┬───────┘
  │      │      │      │      │      │
  │      │      │      │      │      │
  ▼      ▼      ▼      ▼      ▼      ▼
[User] [Acc] [Trd] [AI] [Scr] [Nginx]
Service Service Service Service Service Logs

각 서비스: JSON 형식 로그 출력
```

---

### 17.3.2 Logback 설정 (Spring Boot)

**modules/user/api/src/main/resources/logback-spring.xml:**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <springProperty scope="context" name="APP_NAME" source="spring.application.name"/>
    <springProperty scope="context" name="APP_ENV" source="spring.profiles.active"/>

    <!-- 콘솔 출력 -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"app":"${APP_NAME}","env":"${APP_ENV}"}</customFields>
            <includeMdcKeyName>traceId</includeMdcKeyName>
            <includeMdcKeyName>userId</includeMdcKeyName>
        </encoder>
    </appender>

    <!-- 파일 출력 -->
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/var/log/lktrade/${APP_NAME}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>/var/log/lktrade/${APP_NAME}.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <totalSizeCap>10GB</totalSizeCap>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"app":"${APP_NAME}","env":"${APP_ENV}"}</customFields>
        </encoder>
    </appender>

    <!-- 로그 레벨 -->
    <logger name="com.lk.trade" level="DEBUG"/>
    <logger name="org.springframework" level="INFO"/>
    <logger name="org.hibernate" level="WARN"/>

    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="FILE"/>
    </root>
</configuration>
```

**build.gradle.kts에 의존성 추가:**

```kotlin
dependencies {
    // Logstash Encoder (JSON 로그)
    implementation("net.logstash.logback:logstash-logback-encoder:7.4")
}
```

---

### 17.3.3 docker-compose.logging.yml

```yaml
version: '3.8'

services:
  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: lktrade-elasticsearch
    profiles: ["logging"]
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - lktrade-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: lktrade-logstash
    profiles: ["logging"]
    ports:
      - "5000:5000"
      - "5044:5044"
      - "9600:9600"
    volumes:
      - ./docker/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./docker/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
    networks:
      - lktrade-network
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: lktrade-kibana
    profiles: ["logging"]
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      ELASTICSEARCH_USERNAME: elastic
      ELASTICSEARCH_PASSWORD: ${ELASTIC_PASSWORD:-changeme}
    ports:
      - "5601:5601"
    networks:
      - lktrade-network
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Filebeat (로그 수집)
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: lktrade-filebeat
    profiles: ["logging"]
    user: root
    command: filebeat -e -strict.perms=false
    volumes:
      - ./docker/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat-data:/usr/share/filebeat/data
    networks:
      - lktrade-network
    depends_on:
      - elasticsearch
      - logstash
    restart: unless-stopped

volumes:
  elasticsearch-data:
    name: lktrade-elasticsearch-data
  filebeat-data:
    name: lktrade-filebeat-data
```

---

### 17.3.4 Logstash 파이프라인

**docker/logstash/pipeline/logstash.conf:**

```conf
input {
  # Filebeat에서 로그 수집
  beats {
    port => 5044
  }

  # TCP로 직접 수신 (애플리케이션에서 직접 전송)
  tcp {
    port => 5000
    codec => json_lines
  }
}

filter {
  # JSON 파싱
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }

  # 타임스탬프 파싱
  date {
    match => ["@timestamp", "ISO8601"]
    target => "@timestamp"
  }

  # 필드 추가
  mutate {
    add_field => {
      "[@metadata][index]" => "lktrade-logs-%{+YYYY.MM.dd}"
    }
  }

  # 에러 로그 분류
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
    }
  }

  # 트레이드 로그 분류
  if [logger_name] =~ /trade/ {
    mutate {
      add_tag => ["trade"]
      add_field => {
        "[@metadata][index]" => "lktrade-trade-logs-%{+YYYY.MM.dd}"
      }
    }
  }
}

output {
  # Elasticsearch로 전송
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index]}"
    document_type => "_doc"
  }

  # 디버깅용 (개발 환경)
  if [@metadata][env] == "dev" {
    stdout {
      codec => rubydebug
    }
  }
}
```

---

### 17.3.5 Filebeat 설정

**docker/filebeat/filebeat.yml:**

```yaml
filebeat.inputs:
  # Docker 컨테이너 로그
  - type: container
    paths:
      - '/var/lib/docker/containers/*/*.log'

    # 멀티라인 로그 처리
    multiline.type: pattern
    multiline.pattern: '^\d{4}-\d{2}-\d{2}'
    multiline.negate: true
    multiline.match: after

    # 메타데이터 추가
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"

      - decode_json_fields:
          fields: ["message"]
          target: ""
          overwrite_keys: true

# Logstash로 전송
output.logstash:
  hosts: ["logstash:5044"]

# 로깅 레벨
logging.level: info
```

---

## 17.4 실행 및 사용

### 17.4.1 모니터링 스택 실행

```bash
# 모니터링 포함 실행
docker compose --profile monitoring up -d

# 또는 Makefile
make up-monitoring
```

**Makefile 추가:**

```makefile
# 모니터링 시작
up-monitoring:
	docker compose --profile monitoring up -d

# 로깅 시작
up-logging:
	docker compose --profile logging up -d

# 모든 프로파일 시작
up-all:
	docker compose --profile monitoring --profile logging --profile tools up -d

# Prometheus 접속
prometheus:
	@echo "Opening Prometheus: http://localhost:9090"
	@xdg-open http://localhost:9090 2>/dev/null || open http://localhost:9090 2>/dev/null || echo "http://localhost:9090"

# Grafana 접속
grafana:
	@echo "Opening Grafana: http://localhost:3001"
	@echo "Default credentials: admin / admin"
	@xdg-open http://localhost:3001 2>/dev/null || open http://localhost:3001 2>/dev/null || echo "http://localhost:3001"

# Kibana 접속
kibana:
	@echo "Opening Kibana: http://localhost:5601"
	@xdg-open http://localhost:5601 2>/dev/null || open http://localhost:5601 2>/dev/null || echo "http://localhost:5601"
```

---

### 17.4.2 Grafana 대시보드 설정

**1. Prometheus 데이터소스 확인:**
- http://localhost:3001 접속
- Configuration → Data Sources → Prometheus
- Test 버튼 클릭

**2. 대시보드 Import:**
```
Dashboards → Import

추천 대시보드:
- 6417: Spring Boot Statistics
- 4701: JVM (Micrometer)
- 11159: Docker Container & Host Metrics
- 11074: Node Exporter Full
- 9628: PostgreSQL Database
- 763: Redis Dashboard
- 12708: Nginx Ingress Controller
```

---

### 17.4.3 커스텀 메트릭 추가

**UserService.kt 예시:**

```kotlin
package com.lk.trade.user.service

import io.micrometer.core.instrument.Counter
import io.micrometer.core.instrument.MeterRegistry
import io.micrometer.core.instrument.Timer
import org.springframework.stereotype.Service

@Service
class UserService(
    private val meterRegistry: MeterRegistry
) {
    // 카운터 (누적 값)
    private val userRegistrationCounter: Counter = Counter.builder("user.registration.count")
        .description("Total number of user registrations")
        .tag("service", "user")
        .register(meterRegistry)

    // 타이머 (실행 시간)
    private val loginTimer: Timer = Timer.builder("user.login.duration")
        .description("Time taken to process login")
        .tag("service", "user")
        .register(meterRegistry)

    fun registerUser(userData: UserRegistrationDto): User {
        // 등록 횟수 증가
        userRegistrationCounter.increment()

        // 비즈니스 로직
        val user = createUser(userData)

        return user
    }

    fun login(credentials: LoginDto): AuthToken {
        // 실행 시간 측정
        return loginTimer.recordCallable {
            // 로그인 로직
            authenticateUser(credentials)
        } ?: throw AuthenticationException()
    }
}
```

**Grafana에서 쿼리:**

```promql
# 사용자 등록 속도 (분당)
rate(user_registration_count_total[5m]) * 60

# 로그인 평균 응답 시간
rate(user_login_duration_seconds_sum[5m])
/
rate(user_login_duration_seconds_count[5m])

# P95 로그인 응답 시간
histogram_quantile(0.95, rate(user_login_duration_seconds_bucket[5m]))
```

---

## 👨‍💻 주니어 시나리오

### 시나리오 1: Prometheus가 메트릭을 수집하지 못함

**상황**: 모니터링 스택을 구축했는데 Prometheus 대시보드에서 "No data" 표시

```yaml
# ❌ 주니어 개발자가 작성한 prometheus.yml
scrape_configs:
  - job_name: 'user-service'
    metrics_path: '/metrics'  # 잘못된 경로
    static_configs:
      - targets: ['localhost:8081']  # 잘못된 호스트
```

**문제점**:
- 문제 1: Spring Boot Actuator의 기본 경로는 `/actuator/prometheus`인데 `/metrics`로 설정
- 문제 2: Docker 네트워크에서는 `localhost`가 아닌 서비스명 사용
- 왜 이 문제가 발생하는가: Docker 컨테이너는 각자 격리된 네트워크를 가지므로 localhost는 자기 자신을 가리킴

**해결책**:
```yaml
# ✅ 올바른 코드
scrape_configs:
  - job_name: 'user-service'
    metrics_path: '/actuator/prometheus'  # 올바른 경로
    static_configs:
      - targets: ['user-service:8081']  # Docker 서비스명 사용

# 설명
# 1. Spring Boot Actuator는 /actuator/prometheus 경로에 메트릭 노출
# 2. Docker Compose에서는 서비스명이 DNS로 해석됨
```

**배운 점**:
- 💡 팁 1: Prometheus 타겟 상태는 http://localhost:9090/targets 에서 확인
- 💡 팁 2: `curl http://user-service:8081/actuator/prometheus`로 메트릭 직접 확인
- 💡 팁 3: Docker 네트워크 내에서는 항상 서비스명 사용

### 시나리오 2: Grafana 대시보드에 그래프가 안 나옴

**상황**: Grafana에서 PromQL 쿼리를 작성했는데 "No data points" 에러

```promql
# ❌ 잘못된 PromQL
rate(http_requests[5m])  # 메트릭명 틀림
```

**문제점**:
- 문제 1: 정확한 메트릭명을 모름 (http_requests vs http_server_requests_seconds_count)
- 문제 2: rate() 함수는 counter 타입에만 사용 가능
- 문제 3: 시간 범위가 데이터 수집 주기보다 작을 수 있음

**해결책**:
```promql
# ✅ 올바른 쿼리

# 1. 메트릭명 확인 (Prometheus에서 직접 확인)
http_server_requests_seconds_count

# 2. 5분간의 초당 요청 수
rate(http_server_requests_seconds_count[5m])

# 3. 서비스별 필터링
rate(http_server_requests_seconds_count{service="user"}[5m])

# 설명:
# - rate()는 counter의 증가율 계산
# - [5m]은 5분 동안의 데이터 사용
# - {}는 레이블 필터링
```

**배운 점**:
- 💡 팁 1: Prometheus UI의 "Graph" 탭에서 메트릭명 자동완성 활용
- 💡 팁 2: `{__name__=~"http.*"}`로 메트릭명 검색
- 💡 팁 3: rate()는 최소 2개 이상의 데이터 포인트 필요

### 시나리오 3: 로그가 Elasticsearch에 저장 안 됨

**상황**: 애플리케이션은 로그를 출력하는데 Kibana에서 검색이 안 됨

```xml
<!-- ❌ 잘못된 logback-spring.xml -->
<appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
        <pattern>%d{HH:mm:ss} - %msg%n</pattern>  <!-- 평문 로그 -->
    </encoder>
</appender>
```

**문제점**:
- 문제 1: JSON 형식이 아닌 평문 로그 출력
- 문제 2: Logstash가 파싱할 수 없는 형식
- 문제 3: 구조화되지 않은 로그는 검색/분석 어려움

**해결책**:
```xml
<!-- ✅ 올바른 설정 -->
<appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
    <encoder class="net.logstash.logback.encoder.LogstashEncoder">
        <customFields>{"app":"user-service","env":"dev"}</customFields>
        <includeMdcKeyName>traceId</includeMdcKeyName>
    </encoder>
</appender>

<!-- build.gradle.kts에 의존성 추가 -->
<!-- implementation("net.logstash.logback:logstash-logback-encoder:7.4") -->
```

**배운 점**:
- 💡 팁 1: JSON 로그는 구조화되어 검색/필터링이 쉬움
- 💡 팁 2: MDC(Mapped Diagnostic Context)로 traceId 추가하면 요청 추적 가능
- 💡 팁 3: Logstash 파이프라인에서 JSON 파싱 확인: `codec => json_lines`

### 시나리오 4: 알림이 너무 많이 옴 (Alert Fatigue)

**상황**: Slack에 알림이 5분마다 수백 개씩 와서 중요한 알림 놓침

```yaml
# ❌ 너무 민감한 알림 규칙
- alert: HighCpuUsage
  expr: container_cpu_usage > 50  # 너무 낮은 임계값
  for: 10s  # 너무 짧은 지속 시간
  labels:
    severity: critical  # 모든 알림을 critical로
```

**문제점**:
- 문제 1: CPU 50% 초과는 정상 범위인데 알림 발생
- 문제 2: 10초만 지속되어도 알림 (일시적 스파이크)
- 문제 3: 모든 알림을 critical로 설정하여 우선순위 없음

**해결책**:
```yaml
# ✅ 적절한 알림 규칙
- alert: HighCpuUsage
  expr: container_cpu_usage > 80  # 적절한 임계값
  for: 5m  # 5분 지속 시에만 알림
  labels:
    severity: warning  # 단계별 심각도
  annotations:
    summary: "CPU 사용률 높음: {{ $labels.container }}"
    description: "5분간 80% 초과"

- alert: CriticalCpuUsage
  expr: container_cpu_usage > 95
  for: 2m
  labels:
    severity: critical  # 정말 심각한 경우만
```

**배운 점**:
- 💡 팁 1: 임계값은 실제 운영 데이터 기반으로 조정
- 💡 팁 2: for 시간을 설정하여 일시적 스파이크 무시
- 💡 팁 3: severity를 info/warning/critical로 구분하여 우선순위 관리
- 💡 팁 4: Alertmanager의 inhibit_rules로 중복 알림 방지

---

## ❓ FAQ

<details>
<summary><strong>Q1: Prometheus와 Grafana의 차이가 뭔가요?</strong></summary>

**A**: Prometheus는 메트릭을 수집하고 저장하는 데이터베이스이고, Grafana는 그 데이터를 예쁜 그래프로 보여주는 시각화 도구입니다.

**상세 설명**:
- Prometheus: 데이터 수집, 저장, PromQL 쿼리, 알림 규칙
- Grafana: 대시보드, 그래프, 여러 데이터소스 통합

**예시**:
```bash
# Prometheus는 이렇게 사용
curl http://localhost:9090/api/v1/query?query=up

# Grafana는 웹 UI로 시각화
http://localhost:3001/dashboards
```

**실무 팁**:
💡 Prometheus는 "데이터", Grafana는 "보여주기"로 역할 분담

</details>

<details>
<summary><strong>Q2: 메트릭 수집 주기는 어떻게 설정하나요?</strong></summary>

**A**: `prometheus.yml`의 `scrape_interval`로 설정하며, 기본값은 15초입니다.

**상세 설명**:
- 짧을수록: 더 정확하지만 리소스 많이 사용
- 길수록: 리소스 절약하지만 정확도 감소
- 실무 권장: 15초 (CPU/메모리 정상), 5초 (고성능 요구), 1분 (배치 작업)

**예시**:
```yaml
global:
  scrape_interval: 15s       # 전역 설정
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'fast-service'
    scrape_interval: 5s      # 개별 설정 (전역 덮어쓰기)
```

**실무 팁**:
💡 중요한 서비스는 짧게, 배치 작업은 길게 설정

</details>

<details>
<summary><strong>Q3: 로그는 얼마나 보관해야 하나요?</strong></summary>

**A**: 법적 요구사항과 디스크 용량에 따라 다르지만, 일반적으로 30-90일 권장합니다.

**상세 설명**:
- 개발 환경: 7-14일
- 스테이징: 30일
- 프로덕션: 90일 이상
- 규제 업종: 1-7년 (금융, 의료)

**예시**:
```yaml
# Elasticsearch 인덱스 수명 주기
PUT _ilm/policy/lktrade-logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_age": "1d",
            "max_size": "50GB"
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

**실무 팁**:
💡 Hot 데이터는 SSD에, Cold 데이터는 저렴한 스토리지로 이동

</details>

<details>
<summary><strong>Q4: PromQL이 어려운데 어떻게 배우나요?</strong></summary>

**A**: Prometheus UI에서 실시간으로 실험하면서 배우는 것이 가장 효과적입니다.

**상세 설명**:
1. 기본 메트릭 조회: `up`, `http_requests_total`
2. 레이블 필터링: `{job="user-service"}`
3. 함수 사용: `rate()`, `sum()`, `avg()`
4. 시간 범위: `[5m]`, `[1h]`

**예시**:
```promql
# 초급: 서비스 상태 확인
up{job="user-service"}

# 중급: 5분간 평균 RPS
rate(http_requests_total[5m])

# 고급: 서비스별 95% 응답 시간
histogram_quantile(0.95,
  rate(http_request_duration_seconds_bucket[5m])
)
```

**실무 팁**:
💡 Prometheus UI의 "Graph" 탭에서 자동완성 활용

</details>

<details>
<summary><strong>Q5: 모니터링 스택이 너무 무거운데 경량화 방법은?</strong></summary>

**A**: Profile을 활용하여 필요한 컴포넌트만 실행하거나, VictoriaMetrics 같은 경량 대안을 사용하세요.

**상세 설명**:
- 리소스 사용량 (LK-Trade 기준):
  - Prometheus: CPU 200m, RAM 512MB
  - Grafana: CPU 100m, RAM 256MB
  - Elasticsearch: CPU 1000m, RAM 2GB (가장 무거움)

**경량화 방법**:
```yaml
# 1. 필요한 것만 실행
docker compose --profile monitoring up -d  # 로깅 제외

# 2. Elasticsearch 대신 Loki 사용 (로그 전용, 경량)
# 3. 오래된 메트릭/로그 삭제
```

**실무 팁**:
💡 개발 환경에서는 모니터링만, 운영 환경에서는 전체 스택 실행

</details>

<details>
<summary><strong>Q6: Slack 알림 대신 이메일로 받을 수 있나요?</strong></summary>

**A**: Alertmanager에서 email_configs를 설정하면 가능합니다.

**상세 설명**:
- Slack: 실시간 알림, 팀 협업
- Email: 개인 알림, 상세 리포트
- PagerDuty: 온콜 관리, 에스컬레이션

**예시**:
```yaml
# alertmanager.yml
receivers:
  - name: 'email-alerts'
    email_configs:
      - to: 'ops@company.com'
        from: 'alertmanager@company.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alerts@company.com'
        auth_password: 'your-password'
        headers:
          Subject: '[LK-Trade] {{ .GroupLabels.alertname }}'
```

**실무 팁**:
💡 Critical은 Slack + SMS, Warning은 Email, Info는 로그만

</details>

<details>
<summary><strong>Q7: Grafana 대시보드를 코드로 관리할 수 있나요?</strong></summary>

**A**: 가능합니다. JSON 파일로 내보내기/가져오기하거나 Provisioning을 사용하세요.

**상세 설명**:
- Dashboard JSON Export: 수동 백업
- Provisioning: 자동 배포 (권장)
- Terraform: IaC로 관리

**예시**:
```yaml
# docker/grafana/provisioning/dashboards/default.yml
apiVersion: 1

providers:
  - name: 'Default'
    folder: ''
    type: file
    options:
      path: /var/lib/grafana/dashboards

# docker-compose.yml
volumes:
  - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
```

**실무 팁**:
💡 대시보드 JSON을 Git에 커밋하여 버전 관리

</details>

---

## 💼 면접 질문 리스트

### 📘 주니어/신입 개발자용 (5-7개)

<details>
<summary><strong>1. Prometheus와 ELK의 차이를 설명해주세요.</strong></summary>

**모범 답안 포인트**
- Prometheus: 메트릭(숫자) 수집, 시계열 데이터베이스, Pull 방식
- ELK: 로그(텍스트) 수집, 검색 엔진, Push 방식
- 용도: Prometheus는 "지금 상태", ELK는 "왜 그런지 분석"

**예시 답변**
> "Prometheus는 CPU, 메모리 같은 숫자 데이터를 수집하여 시스템 상태를 모니터링하고, ELK는 애플리케이션 로그를 수집하여 문제 원인을 분석합니다. Prometheus는 Pull 방식으로 주기적으로 메트릭을 가져오고, ELK는 Push 방식으로 로그를 받아서 저장합니다."

**꼬리 질문**
- Q: Pull 방식과 Push 방식의 장단점은?
- A: Pull은 타겟 관리가 쉽고 서버 부하 조절 가능, Push는 실시간성이 좋고 방화벽 통과 쉬움

**실무 연관**
- 둘을 함께 사용하여 "무엇이 문제인지"와 "왜 문제인지"를 모두 파악

</details>

<details>
<summary><strong>2. 4가지 골든 시그널(Golden Signals)이 무엇인가요?</strong></summary>

**모범 답안 포인트**
- Latency (지연 시간): 응답 속도
- Traffic (트래픽): 요청 수
- Errors (에러율): 실패율
- Saturation (포화도): 리소스 사용률

**예시 답변**
> "구글 SRE에서 제안한 4가지 핵심 모니터링 지표입니다. Latency는 요청 처리 시간, Traffic은 초당 요청 수, Errors는 실패율, Saturation은 CPU/메모리 사용률입니다. 이 4가지만 잘 모니터링해도 대부분의 문제를 사전에 발견할 수 있습니다."

**꼬리 질문**
- Q: 어느 것이 가장 중요한가요?
- A: 서비스 특성마다 다르지만 일반적으로 Errors가 가장 치명적

**실무 연관**
- 대시보드 첫 화면에 이 4가지를 배치하여 한눈에 파악

</details>

<details>
<summary><strong>3. Grafana 대시보드에서 데이터가 안 보일 때 어떻게 확인하나요?</strong></summary>

**모범 답안 포인트**
- 데이터소스 연결 확인
- PromQL 쿼리 문법 확인
- 시간 범위 확인
- Prometheus 타겟 상태 확인

**예시 답변**
> "먼저 Grafana 설정에서 데이터소스가 Prometheus에 잘 연결되어 있는지 Test 버튼으로 확인합니다. 그 다음 PromQL 쿼리가 올바른지 Prometheus UI에서 직접 테스트하고, Grafana의 시간 범위가 데이터가 있는 기간과 일치하는지 확인합니다. 마지막으로 Prometheus의 /targets에서 스크래핑 대상이 UP 상태인지 확인합니다."

**꼬리 질문**
- Q: 메트릭명을 모를 때는?
- A: Prometheus UI에서 `{__name__=~".*"}`로 모든 메트릭 조회

**실무 연관**
- 문제 해결 순서: 연결 → 쿼리 → 데이터 → 타겟

</details>

<details>
<summary><strong>4. 로그를 JSON 형식으로 출력하는 이유는?</strong></summary>

**모범 답안 포인트**
- 구조화된 데이터로 파싱 쉬움
- 필드별 검색/필터링 가능
- 자동화 분석에 유리
- 표준 형식으로 도구 연동 쉬움

**예시 답변**
> "평문 로그는 사람이 읽기 쉽지만 프로그램이 파싱하기 어렵습니다. JSON 형식은 timestamp, level, message 같은 필드가 명확히 구분되어 Logstash나 Elasticsearch가 쉽게 파싱하고, Kibana에서 필드별로 검색할 수 있습니다. 또한 traceId 같은 커스텀 필드를 추가하여 분산 추적도 가능합니다."

**꼬리 질문**
- Q: JSON의 단점은?
- A: 사람이 읽기 어렵고, 파일 크기가 평문보다 큼

**실무 연관**
- 개발 환경은 평문, 운영 환경은 JSON 권장

</details>

<details>
<summary><strong>5. Alertmanager의 역할은 무엇인가요?</strong></summary>

**모범 답안 포인트**
- Prometheus 알림 규칙 처리
- 중복 알림 제거 (Deduplication)
- 알림 그룹화 (Grouping)
- 알림 라우팅 (Routing)
- 외부 전송 (Slack, Email 등)

**예시 답변**
> "Prometheus가 알림 규칙을 평가하여 문제를 감지하면, Alertmanager가 그 알림을 받아서 처리합니다. 같은 알림이 여러 번 오면 중복 제거하고, 관련된 알림들을 그룹화하며, 심각도에 따라 적절한 채널(Slack, Email 등)로 전송합니다."

**꼬리 질문**
- Q: 알림이 너무 많이 올 때는?
- A: for 시간 늘리기, 임계값 조정, inhibit_rules로 중복 억제

**실무 연관**
- Critical은 즉시 Slack, Warning은 5분 그룹화 후 전송

</details>

<details>
<summary><strong>6. Docker 환경에서 로그 수집은 어떻게 하나요?</strong></summary>

**모범 답안 포인트**
- Filebeat가 Docker 로그 수집
- `/var/lib/docker/containers/`에서 읽음
- Docker 메타데이터 자동 추가
- Logstash로 전송하여 파싱

**예시 답변**
> "Filebeat를 Docker 컨테이너로 실행하고, `/var/lib/docker/containers` 디렉토리를 마운트하여 모든 컨테이너의 로그를 수집합니다. Filebeat는 Docker 메타데이터(컨테이너명, 이미지 등)를 자동으로 추가하고, Logstash로 전송하여 JSON 파싱 후 Elasticsearch에 저장합니다."

**꼬리 질문**
- Q: stdout/stderr 로그만 수집되나요?
- A: 맞습니다. 파일로 저장하는 로그는 볼륨 마운트 필요

**실무 연관**
- 컨테이너 재시작 시 로그 유실 방지를 위해 중앙 집중형 로깅 필수

</details>

<details>
<summary><strong>7. PromQL에서 rate()와 irate()의 차이는?</strong></summary>

**모범 답안 포인트**
- rate(): 시간 범위 내 평균 증가율
- irate(): 마지막 두 샘플의 즉시 증가율
- rate()는 부드러운 그래프, irate()는 민감한 그래프

**예시 답변**
> "rate()는 지정한 시간 범위(예: 5분) 동안의 평균 증가율을 계산하여 부드러운 그래프를 그립니다. irate()는 마지막 두 샘플만 사용하여 즉시 증가율을 계산하므로 최신 변화를 민감하게 반영합니다. 일반적으로 알림 규칙에는 rate(), 실시간 모니터링에는 irate()를 사용합니다."

**꼬리 질문**
- Q: 언제 어느 것을 써야 하나요?
- A: 장기 트렌드는 rate(), 급격한 변화 감지는 irate()

**실무 연관**
- rate(http_requests[5m])는 5분 평균 RPS, irate()는 최근 15-30초 RPS

</details>

---

### 📗 중급 개발자용 (3-5개)

<details>
<summary><strong>1. Prometheus의 Pull 방식이 Push 방식보다 나은 이유는?</strong></summary>

**모범 답안 포인트**
- 타겟 관리 중앙화 (Prometheus가 제어)
- 서버 부하 조절 가능 (scrape_interval 조정)
- 헬스 체크 통합 (타겟 다운 감지)
- 방화벽 설정 단순 (inbound만 열면 됨)

**예시 답변**
> "Pull 방식은 Prometheus가 능동적으로 메트릭을 수집하므로 어떤 타겟을 모니터링할지 중앙에서 관리할 수 있습니다. 또한 scrape_interval을 조정하여 서버 부하를 제어하고, 타겟이 응답하지 않으면 즉시 감지하여 알림을 보낼 수 있습니다. Push 방식은 각 서비스가 직접 전송하므로 타겟 관리가 분산되고, 네트워크 장애 시 데이터 유실 가능성이 높습니다."

**실무 예시**:
```yaml
# Prometheus가 타겟 관리
scrape_configs:
  - job_name: 'production'
    scrape_interval: 15s
  - job_name: 'batch'
    scrape_interval: 1m  # 배치는 덜 자주
```

**꼬리 질문**
- Q: Push가 필요한 경우는?
- A: 단기 실행 작업(Batch), 방화벽 뒤 서비스 → Pushgateway 사용

**실무 연관**
- 마이크로서비스 환경에서 수백 개 서비스를 중앙에서 통제

</details>

<details>
<summary><strong>2. Cardinality 문제와 해결 방법을 설명해주세요.</strong></summary>

**모범 답안 포인트**
- Cardinality: 고유한 시계열 개수
- 높은 Cardinality → 메모리/디스크 폭증
- 원인: userId, requestId 같은 high-cardinality 레이블
- 해결: 레이블 최소화, relabel_configs로 제거

**예시 답변**
> "Cardinality는 메트릭과 레이블 조합으로 만들어지는 고유한 시계열의 개수입니다. 예를 들어 userId를 레이블로 추가하면 사용자 100만 명당 100만 개의 시계열이 생성되어 Prometheus 메모리가 부족해집니다. 해결 방법은 userId 같은 high-cardinality 레이블을 제거하고, 필요하면 로그로 기록하거나 별도 분석 시스템을 사용하는 것입니다."

**실무 예시**:
```yaml
# ❌ 나쁜 예: userId 레이블 (high-cardinality)
http_requests{userId="12345"}

# ✅ 좋은 예: 집계된 메트릭
http_requests{service="user", status="200"}

# relabel_configs로 레이블 제거
metric_relabel_configs:
  - source_labels: [userId]
    action: labeldrop
```

**꼬리 질문**
- Q: Cardinality를 확인하는 방법은?
- A: `count({__name__=~".+"})`로 총 시계열 개수 확인

**실무 연관**
- Cardinality 폭증으로 Prometheus OOM 발생 → 운영 장애

</details>

<details>
<summary><strong>3. ELK의 각 컴포넌트 역할과 데이터 흐름을 설명해주세요.</strong></summary>

**모범 답안 포인트**
- Filebeat: 로그 수집 (경량 에이전트)
- Logstash: 파싱, 변환, 필터링
- Elasticsearch: 인덱싱, 저장, 검색
- Kibana: 시각화, 대시보드

**예시 답변**
> "Filebeat가 각 서버/컨테이너에서 로그를 수집하여 Logstash로 전송합니다. Logstash는 로그를 파싱(JSON 변환)하고 필터링(불필요한 필드 제거)한 후 Elasticsearch에 전송합니다. Elasticsearch는 로그를 인덱싱하여 저장하고, Kibana는 Elasticsearch를 데이터소스로 사용하여 로그를 검색하고 시각화합니다."

**실무 예시**:
```
[Container] → stdout → [Filebeat] → [Logstash] → [Elasticsearch] → [Kibana]
  로그 출력      수집       파싱/변환      인덱싱/저장      검색/시각화

Filebeat: 평문 로그 읽기
Logstash: JSON 파싱, 필드 추출, 타임스탬프 정규화
Elasticsearch: 인덱스 lktrade-logs-2024.01.01에 저장
Kibana: "error" 키워드로 검색
```

**꼬리 질문**
- Q: Logstash를 건너뛰고 Filebeat → Elasticsearch 직접 전송 가능한가요?
- A: 가능하지만 복잡한 파싱은 어렵고, Logstash의 풍부한 필터 기능 사용 불가

**실무 연관**
- 대규모 환경에서는 Kafka를 중간에 두어 버퍼링 (안정성 향상)

</details>

<details>
<summary><strong>4. 분산 추적(Distributed Tracing)과 모니터링/로깅의 관계는?</strong></summary>

**모범 답안 포인트**
- 모니터링: 각 서비스의 상태 (개별 점)
- 로깅: 각 서비스의 이벤트 (개별 선)
- 분산 추적: 요청의 전체 흐름 (연결된 선)
- TraceID로 연결

**예시 답변**
> "모니터링은 각 서비스의 CPU, 메모리를 개별적으로 추적하고, 로깅은 각 서비스의 로그를 기록합니다. 하지만 마이크로서비스 환경에서는 하나의 요청이 여러 서비스를 거치므로, 분산 추적으로 전체 흐름을 파악해야 합니다. TraceID를 모든 로그에 포함시키면 Kibana에서 TraceID로 검색하여 요청의 전체 여정을 추적할 수 있습니다."

**실무 예시**:
```
사용자 요청 (TraceID: abc123)
  ↓
API Gateway (TraceID: abc123, SpanID: 1)
  ↓
User Service (TraceID: abc123, SpanID: 2)
  ↓
Account Service (TraceID: abc123, SpanID: 3)

Kibana 검색: TraceID:abc123
→ 전체 로그 한 번에 조회
```

**꼬리 질문**
- Q: 분산 추적 도구는?
- A: Jaeger, Zipkin, OpenTelemetry

**실무 연관**
- TraceID 없으면 장애 원인 찾는데 몇 시간 소요, 있으면 몇 분

</details>

<details>
<summary><strong>5. Prometheus Recording Rule과 Alert Rule의 차이와 활용법은?</strong></summary>

**모범 답안 포인트**
- Recording Rule: 자주 쓰는 복잡한 쿼리를 미리 계산하여 저장
- Alert Rule: 조건 만족 시 알림 발송
- Recording Rule로 성능 최적화, Alert Rule로 장애 대응

**예시 답변**
> "Recording Rule은 복잡한 PromQL 쿼리를 주기적으로 실행하여 결과를 새 메트릭으로 저장합니다. 예를 들어 P95 응답 시간을 매번 계산하는 대신 Recording Rule로 미리 계산하여 Grafana 로딩 속도를 높입니다. Alert Rule은 특정 조건(예: CPU > 80%)을 만족하면 Alertmanager에 알림을 보냅니다."

**실무 예시**:
```yaml
# Recording Rule: 자주 쓰는 쿼리 사전 계산
groups:
  - name: performance
    interval: 30s
    rules:
      - record: job:http_requests:rate5m
        expr: rate(http_requests_total[5m])

      - record: job:http_latency:p95
        expr: histogram_quantile(0.95, rate(http_duration_bucket[5m]))

# Alert Rule: 조건 만족 시 알림
      - alert: HighLatency
        expr: job:http_latency:p95 > 1
        for: 5m
```

**꼬리 질문**
- Q: Recording Rule의 단점은?
- A: 저장 공간 증가, 실시간성 약간 감소 (interval 만큼 지연)

**실무 연관**
- 복잡한 대시보드는 Recording Rule로 성능 최적화 필수

</details>

---

## 📝 핵심 정리

### 이 장에서 배운 핵심 개념

| 개념 | 설명 | 핵심 키워드 |
|------|------|-------------|
| Prometheus | 메트릭 수집 및 시계열 DB | Pull 방식, PromQL, 알림 규칙 |
| Grafana | 메트릭 시각화 도구 | 대시보드, 그래프, 여러 데이터소스 |
| Alertmanager | 알림 처리 및 전송 | 중복 제거, 그룹화, 라우팅 |
| ELK Stack | 로그 수집/분석 시스템 | Filebeat, Logstash, Elasticsearch, Kibana |
| 4 Golden Signals | 핵심 모니터링 지표 | Latency, Traffic, Errors, Saturation |
| PromQL | Prometheus 쿼리 언어 | rate(), sum(), histogram_quantile() |
| Cardinality | 고유 시계열 개수 | 메모리 사용량, 레이블 최소화 |
| 분산 추적 | 요청 전체 흐름 추적 | TraceID, SpanID |

### 필수 명령어/코드 정리

| 명령어/코드 | 용도 | 예시 |
|-------------|------|------|
| `docker compose --profile monitoring up -d` | 모니터링 스택 실행 | Prometheus + Grafana 시작 |
| `http://localhost:9090/targets` | Prometheus 타겟 확인 | 스크래핑 상태 점검 |
| `rate(http_requests[5m])` | 5분간 초당 요청 수 | PromQL 기본 쿼리 |
| `histogram_quantile(0.95, ...)` | P95 응답 시간 | 성능 분석 |
| `curl http://service:8081/actuator/prometheus` | 메트릭 직접 확인 | 디버깅 |
| Kibana: `level:ERROR` | 에러 로그 검색 | 장애 분석 |
| Logstash JSON 파싱 | 구조화된 로그 | 필드별 검색 |

### 실무 베스트 프랙티스

#### ✅ 해야 할 것
- [ ] 4 Golden Signals를 대시보드 첫 화면에 배치
- [ ] 알림 규칙에 `for` 시간을 설정하여 일시적 스파이크 무시
- [ ] 로그를 JSON 형식으로 출력하여 구조화
- [ ] TraceID를 모든 로그에 포함하여 분산 추적 가능하게
- [ ] Recording Rule로 복잡한 쿼리 사전 계산
- [ ] 심각도별(info/warning/critical) 알림 채널 분리
- [ ] Prometheus 타겟 상태를 주기적으로 점검

#### ❌ 하지 말아야 할 것
- [ ] userId 같은 high-cardinality 레이블 사용
- [ ] 모든 알림을 critical로 설정 (Alert Fatigue)
- [ ] 평문 로그만 사용 (검색/분석 어려움)
- [ ] 로그를 무한정 보관 (디스크 부족)
- [ ] scrape_interval을 너무 짧게 설정 (리소스 낭비)
- [ ] 알림 테스트 없이 프로덕션 배포
- [ ] Grafana 대시보드를 코드로 관리 안 함 (수동 복원 불가)

### 성능/보안 체크리스트

#### 성능
- [ ] Prometheus Cardinality 모니터링 (`count({__name__=~".+"})`)
- [ ] Elasticsearch 인덱스 수명 주기 설정 (ILM)
- [ ] Recording Rule로 복잡한 쿼리 최적화
- [ ] Grafana 대시보드 쿼리 시간 확인 (< 5초)
- [ ] 로그 샘플링 적용 (고트래픽 시)

#### 보안
- [ ] Grafana 기본 비밀번호 변경
- [ ] Prometheus /metrics 엔드포인트 인증 적용
- [ ] Elasticsearch 보안 활성화 (X-Pack)
- [ ] 민감 정보(비밀번호, 토큰) 로그에서 마스킹
- [ ] 알림 채널(Slack Webhook) 환경 변수로 관리

---

## 🔗 관련 기술

**이 기술과 함께 사용하는 기술들**

| 기술 | 관계 | 학습 우선순위 |
|------|------|---------------|
| Jaeger / Zipkin | 분산 추적, TraceID로 연동 | ⭐⭐⭐ |
| OpenTelemetry | 통합 관측성, 표준 수집 | ⭐⭐⭐ |
| Loki | Grafana 로그 시스템, ELK 대안 | ⭐⭐ |
| VictoriaMetrics | Prometheus 호환 경량 DB | ⭐⭐ |
| Thanos | Prometheus 장기 저장소 | ⭐⭐ |
| Fluentd | 로그 수집기, Filebeat 대안 | ⭐⭐ |
| Kafka | 로그 버퍼링, 대규모 환경 | ⭐⭐⭐ |

---

## 🚀 다음 단계

### 다음 장 미리보기: 18장 - CI/CD 파이프라인

Docker 모니터링/로깅을 구축했으니, 이제 자동 배포 시스템을 만들어봅시다!

**배울 내용**:
- **GitHub Actions**: Docker 이미지 자동 빌드/푸시
- **GitLab CI**: 멀티 스테이지 파이프라인
- **Blue-Green 배포**: 무중단 배포 전략
- **카나리 배포**: 점진적 트래픽 전환
- **실전 프로젝트**: LK-Trade 전체 CI/CD 구축

### 이 장과의 연결점
```
17장: 모니터링/로깅으로 시스템 상태 파악
    ↓
18장: CI/CD로 자동 배포 + 배포 모니터링
    ↓
19장: 프로덕션 배포 전략 (보안, 최적화)
    ↓
최종: 완전 자동화된 프로덕션 시스템
```

### 준비하면 좋을 것들
```bash
# GitHub 계정 생성 및 Personal Access Token 발급
# Docker Hub 계정 생성

# GitHub Actions 기본 개념 미리 보기
https://docs.github.com/en/actions

# 다음 장에서 바로 실습할 수 있도록 환경 준비
git init
git remote add origin <your-repo-url>
```

---

## 📚 추가 학습 자료

### 공식 문서
- [Prometheus 공식 문서](https://prometheus.io/docs/)
- [Grafana 대시보드 갤러리](https://grafana.com/grafana/dashboards/)
- [Elastic Stack 가이드](https://www.elastic.co/guide/index.html)
- [PromQL 치트시트](https://promlabs.com/promql-cheat-sheet/)

### 추천 블로그/아티클
- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Prometheus Best Practices (한글)](https://blog.naver.com/prometheus-best-practices)
- [ELK 성능 튜닝 가이드](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html)

### 영상 강의
- [Prometheus & Grafana 완벽 가이드 (유튜브)](https://www.youtube.com/watch?v=prometheus-tutorial)
- [ELK Stack 실전 활용 (인프런)](https://www.inflearn.com/course/elk-stack)

### 오픈소스 프로젝트
- [Awesome Prometheus](https://github.com/roaldnefs/awesome-prometheus)
- [Grafana 대시보드 예제 모음](https://github.com/grafana/grafana-dashboards)

---

## 🎉 축하합니다!

**17장 "모니터링 및 로깅"을 완료했습니다!**

**이제 여러분은**:
✅ Prometheus + Grafana로 시스템 메트릭을 실시간 모니터링할 수 있습니다
✅ ELK 스택으로 로그를 수집하고 분석할 수 있습니다
✅ 4 Golden Signals를 이해하고 대시보드를 구성할 수 있습니다
✅ Alertmanager로 장애 알림 시스템을 구축할 수 있습니다
✅ PromQL로 복잡한 메트릭 쿼리를 작성할 수 있습니다
✅ Cardinality 문제를 이해하고 최적화할 수 있습니다
✅ 분산 추적(TraceID)으로 마이크로서비스를 디버깅할 수 있습니다

**다음 단계**:
- [ ] 18장 "CI/CD 파이프라인"으로 진행하여 자동 배포 시스템 구축
- [ ] 실전 프로젝트: 현재 프로젝트에 모니터링/로깅 적용
- [ ] 면접 질문 복습하여 이론 다지기
- [ ] Grafana 대시보드를 커스터마이징하여 실무 감각 익히기

**실무 팁**:
💡 모니터링은 "구축"보다 "운영"이 중요합니다. 알림 규칙을 지속적으로 튜닝하고, 대시보드를 팀에 맞게 개선하세요!

---

**다음 장으로 이동**: [다음: 18장 CI/CD 파이프라인 →](18-ci-cd.md)

**이전 장으로 돌아가기**: [← 이전: 16장 Docker Swarm](16-docker-swarm.md)

**목차로 돌아가기**: [📚 전체 목차](README.md)